## **VisionZip: Longer is Better but Not Necessary in Vision Language Models**
---
### Background

### Algorithm

æ­¤å¤–ä¹Ÿæ˜¯é‡‡ç”¨äº†mergeï¼Œæ•´ç¯‡æ–‡ç« æ²¡ä»€ä¹ˆçªå‡ºçš„ç®—æ³•æ ¸å¿ƒç‚¹ï¼Œä½†æ˜¯å·¥ç¨‹ã€å®éªŒåšçš„å¾ˆæ‰å®
## **[CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster**
![image]({{ "images/R2UWbLa34oFCItxsmIAcBwchnTc.png" | relURL }})
---
### **Background Analysisï¼š**_**attention shift **_**and **_**attention dispersion**_
1. _**Attention shift: **_** a tendency for textual attention to focus more on later parts of the visual token sequence, which is not desirable for preserving valuable visual information.**


1. _**Attention dispersion:  **_**refers to the less concentrated attention distribution within the LLM compared to the visual encoder.**


### **Methods**
![image]({{ "images/DgQMb6qIco9naKxhiWzc6TAHnWh.png" | relURL }})
æ ¸å¿ƒçš„æƒ³æ³•æ˜¯ç”¨CLS attention æ¥å†³å®špruneæ‰çš„tokenï¼ˆpatchï¼‰ï¼Œç»è¿‡ Visual Encoder ä¹‹åï¼Œå–CLS attentionï¼ŒåR%çš„è¢«pruneæ‰ï¼ˆæåˆ°ä¸€ä¸ªåŠ¨æ€é˜ˆå€¼çš„å…¬å¼ï¼Œæ²¡å•¥ç”¨ï¼‰ï¼Œè®¤ä¸ºè¿™äº›éƒ¨åˆ†çš„patchå¯¹äºæ•´ä½“çš„è¯­ä¹‰è´¡çŒ®å¾ˆä½ã€‚æ•´ä½“å°±æ˜¯ç®€å•çš„æ€è·¯é‡‡ç”¨CLS tokençš„attention scoreå»å¤„ç†ã€‚

è¿™é‡Œçš„é—®é¢˜å°±æ˜¯åˆ°åº•æ˜¯å¦èƒ½çœŸæ­£ä»¥CLS tokençš„attentionå€¼æ¥è¯´æ˜çœŸæ­£çš„importanceç„¶ååšpruneï¼Œè¿™æ ·åšåŸºæœ¬ä¸éœ€è¦ç¡¬ä»¶çš„é¢å¤–æ”¯æŒ
### **Scores**
![image]({{ "images/VOZFbT9N3o9iaBxCQUicNL47nKe.png" | relURL }})
ä»ablationæ¥çœ‹æ•´ä¸ªè®ºæ–‡çš„æ€è·¯åŸºæœ¬éƒ½æ˜¯åœ¨è€ƒè™‘ï¼š
1. pruneçš„ä½ç½®ï¼šLLMæµ…å±‚ OR LLMä¹‹å‰ï¼ˆvisual encoderä¹‹åï¼‰

1. åˆ¤æ–­prune tokençš„æ–¹å¼ï¼š[CLS] token attention OR random OR patch attention

æ²¡æœ‰è€ƒè™‘çš„åœ°æ–¹ï¼š
1. head attention ï¼šåŠ å…¥headçš„å……åˆ†è€ƒè™‘ï¼Œæ¯”å¦‚HeatViTé‚£ç§åŠ å…¥head scoreçš„or so

1. pruneçš„åªèƒ½æ˜¯token-wiseçš„ä¹ˆï¼Ÿæœ‰è®ºæ–‡ä¼šåœ¨dç»´åº¦åšé€‚å½“çš„pruneï¼Œæˆ–è€…headç»´åº¦åšpruneï¼Œä¹Ÿéƒ½æ˜¯ä¸€äº›å¸¸è§çš„æ€è·¯ï¼Œå¦å¤–ç»“æ„åŒ–ç¨€ç–æ€§è‚¯å®šæ˜¯è¦ä¼˜å…ˆè€ƒè™‘çš„ï¼Œæ¯”å¦‚å¯ä»¥ç»†ç²’åº¦ç»“æ„åŒ–ä¹‹ç±»çš„ï¼Œå‚è€ƒsangeréå¸¸ç»å…¸ï¼Œä½†æ˜¯åšçš„ä¹Ÿæ¯”è¾ƒå¤š

1. å¤§å¤šè®ºæ–‡éƒ½é‡‡ç”¨prune+mergeçš„æ¨¡å¼ï¼Œèƒ½ä¸èƒ½æ”¹å˜è¿™ä¸ªæ¨¡å¼ï¼Œèƒ½æŠŠpruneæ‰çš„token informationåˆ©ç”¨èµ·æ¥ï¼ˆå¦‚æœæ˜¯token-wiseçš„è¯ï¼‰æˆ–è€…å……åˆ†åˆ©ç”¨pruneæ‰çš„å…¶ä»–ä¿¡æ¯ï¼Œæ¯”å¦‚ç”¨å¯å­¦ä¹ çš„å°å‹ç½‘ç»œç­‰æ¥å­¦ä¹ ä¸€äº›æ¨¡å¼ï¼ŒæŠŠpruneæ‰çš„ç”¨æ¯”è¾ƒefficientçš„æ–¹å¼æ¢å¤å‡ºæ¥ã€‚

1. æœ‰æ²¡æœ‰å¯èƒ½æ‘†è„±importanceåšpruneçš„é€‰æ‹©ï¼Œæ¯”å¦‚#20DiVpruneæ˜¯ä¸€ä¸ªå¾ˆå–å·§çš„æ–¹å¼

1. text-agnosticè‚¯å®šæ˜¯ä¸€ç§æ›´ä¸ºaccuracy-friendlyçš„ç®—æ³•ï¼Œé‚£ä¹ˆèƒ½ä¸èƒ½æœ‰ä¸€ç§æ–¹æ³•ï¼Œå…ˆpruneæ‰ä¸€éƒ¨åˆ†tokenï¼ˆif token-wise)ç„¶åå†æƒ³åŠæ³•æ ¹æ®text tokenå»æ¢å¤æˆ–è€…æ›´å¥½çš„æå–non-informative tokenæˆ–è€…è¿›ä¸€æ­¥åšæ›´åˆé€‚çš„pruneï¼Œæ˜¯ä¸€ä¸ª2-stageçš„æ–¹æ³•

1. å¯ä»¥è°ƒæŸ¥ä¸€ä¸‹å¯¹äºVLMè®¡ç®—å¤æ‚åº¦æœ€å¤§çš„åœ°æ–¹ï¼ŒäºŒæ¬¡æ–¹çš„attentionè‚¯å®šæ˜¯è¦è§£å†³çš„ï¼Œå¯æ˜¯å¦‚æœèƒ½æœ‰åŠæ³•ä¼˜åŒ–åç»­çš„å¤§è§„æ¨¡çš„FFNé‚£è‚¯å®šæ˜¯æ›´efficientçš„

## **SPARSEVLM: VISUAL TOKEN SPARSIFICATION FOR  EFFICIENT VISION-LANGUAGE MODEL INFERENCE_VVVI**
---
### Background & Target
we propose an efficient training-free token optimization mechanism dubbed SparseVLM without extra parameters or fine-tuning costs. Concretely, given that visual tokens complement text tokens in VLMs for linguistic reasoning, we select visual-relevant text tokens to rate the significance of vision tokens within the self-attention matrix extracted from the VLMs. Then we progressively prune irrelevant tokens.

SparseVLM æ˜¯ä¸€ä¸ªè®­ç»ƒå…ï¼ˆtraining-freeï¼‰ã€æ–‡æœ¬å¼•å¯¼ï¼ˆtext-awareï¼‰çš„è§†è§‰ token ç¨€ç–åŒ–æ¡†æ¶ï¼š
 å…ˆæŒ‘å‡ºâ€œå’Œå›¾åƒå¼ºç›¸å…³â€çš„æ–‡æœ¬ token å½“è¯„å®¡ï¼ˆratersï¼‰ï¼Œç”¨å®ƒä»¬åœ¨è§£ç å™¨é‡Œçš„è·¨æ¨¡æ€æ³¨æ„åŠ›ç»™æ¯ä¸ªè§†è§‰ token æ‰“åˆ†ï¼›ç„¶åé€å±‚æŒ‰æ‰“åˆ†å’Œä¸€ä¸ªç§©ï¼ˆrankï¼‰è‡ªé€‚åº”è§„åˆ™åˆ æ‰ä¸é‡è¦çš„è§†è§‰ tokenï¼›è¢«åˆ çš„ token ä¸ç›´æ¥ä¸¢ï¼Œè€Œæ˜¯å›æ”¶â€”èšç±»â€”é‡æ„æˆå°‘é‡â€œå‹ç¼© tokenâ€å†æ”¾å›ï¼Œå°½é‡ä¸ä¸¢ä¿¡æ¯ã€‚è¿™æ ·èƒ½æ˜¾è‘—å‡ FLOPs/æ˜¾å­˜/æ—¶å»¶ï¼ŒåŒæ—¶ä¿æŒé«˜ç²¾åº¦ã€‚
### Algorithm
éå¸¸å¥½çš„ä¸€ä¸ªtext-agnosticç®—æ³•ï¼Œå¤§æ¦‚ç‡æˆ‘æœ€åä¼šfollowè¿™ä¸ªåšä¸€äº›å·¥ä½œ
#### Raters 
text-awareçš„æ ¸å¿ƒæ˜¯æ ¹æ®text tokensæœ‰é’ˆå¯¹æ€§çš„å¯¹visual tokensåšpruningï¼Œæœ¬æ–‡çš„ä¸€ä¸ªé‡è¦ç®—æ³•æ˜¯é¦–å…ˆåœ¨è¿›å…¥LLMä¹‹å‰è¿›è¡Œä¸€æ¬¡ratersçš„é€‰æ‹©ï¼Œratersæ˜¯ä¸€éƒ¨åˆ†text tokensï¼Œå«åšâ€œè¯„å§”â€ï¼Œé€šå¸¸æ¥è¯´ä¸€å¥textä¸­çœŸæ­£å¯¹å›¾åƒtokené€‰å–æœ‰æŒ‡å¯¼æ„ä¹‰çš„å¹¶æ²¡æœ‰å‡ ä¸ªï¼Œæ¯”å¦‚å¤§é‡çš„å† è¯ï¼Œä»‹è¯ç­‰éƒ½æ˜¯ä¸é‡è¦çš„å¯ä»¥å¿½ç•¥çš„ï¼Œå› æ­¤è¦é€‰æ‹©ratersï¼Œå…ˆä»æ•´å¥æ–‡æœ¬é‡Œè¯„ä¼°æ¯ä¸ªè¯å’Œè§†è§‰çš„å…³è”åº¦$$
R
$$
ï¼Œåªä¿ç•™é«˜äºå¹³å‡å€¼ $$
m=Mean(R)
$$
 çš„å€™é€‰è¯å½“è¯„å§”ï¼Œå‡å°‘æ— å…³è¯å¯¹æ‰“åˆ†çš„å™ªå£°ï¼›è¯„å§”é›†åˆç”¨ç­–ç•¥ $$
S
$$
 ç¡®å®šï¼Œæ•´æ­¥åªåœ¨è¿›å…¥è§£ç å™¨ä¹‹å‰åšä¸€æ¬¡ï¼Œå¼€é”€ $$
O(Lt\ Â· Lv\  Â· D)
$$

#### Rate the visual tokens with scores
å¯¹æ¯ä¸ªè§†è§‰ token jï¼ŒæŠŠè¯„å§”ï¼ˆæ–‡æœ¬ï¼‰å¯¹å®ƒçš„æ³¨æ„åŠ›åˆ†æ•°æŒ‰è¡Œæ±‚å¹³å‡ï¼Œå¾—åˆ°ç¬¬ j ä¸ªè§†è§‰ token çš„ç»¼åˆä»·å€¼åˆ†ï¼Œç„¶åæ ¹æ®è¿™ä¸ªå¾—åˆ†è¿›è¡Œpruneï¼Œå…·ä½“pruneæ‰å¤šå°‘ä¸ªå‘¢ï¼Œç”¨åˆ°äº†ä¸€ä¸ªrank-basedçš„æ–¹æ³•ï¼Œæ¯”å¦‚å¦‚æœPçŸ©é˜µæ˜¯æ¥è¿‘æ»¡ç§©çš„è¯ï¼Œé‚£ä¹ˆå°±è¯´æ˜éƒ½æ˜¯çº¿æ€§æ— å…³çš„ï¼Œå°±åŸºæœ¬ä¸éœ€è¦pruneï¼Œä¿ç•™åŸºæœ¬æ‰€æœ‰visual tokensã€‚å¦‚æœPæ˜¯ä½ç§©çš„è¯ï¼Œé‚£ä¹ˆå°±å¤špruneæ‰ä¸€äº›ï¼Œæ ¹æ®æ³¨æ„åŠ›åˆ†æ•°ã€‚ $$
N=Î±â‹…(Lvâˆ’Rank(P))
$$

#### Merge
ä¹Ÿæ˜¯æŒ‰ç…§åˆ†æ•°mergeï¼Œåšä¸€æ¬¡k è¿‘é‚»å¯†åº¦å³°èšç±»ï¼š


## **MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for  Accelerating Vision-Language Transformer**
---
![image]({{ "images/YRIEbmTgqorENvx2tmDcVrIbnff.png" | relURL }})
### **MAG**


### **DTP**
**äº‹å®è¯æ˜ï¼Œå•æ¨¡æ€å‹ç¼©ä¸­çš„åŠ¨æ€ä»¤ç‰Œä¿®å‰ªæ¯”é™æ€ä»¤ç‰Œä¿®å‰ªæ›´æœ‰æ•ˆï¼Œå› ä¸ºå®ƒå¯ä»¥æ ¹æ®è¾“å…¥å®ä¾‹çš„å¤æ‚ç¨‹åº¦è‡ªé€‚åº”è°ƒæ•´æ¨¡å‹çš„å‹ç¼©ç‡ã€‚**
![image]({{ "images/GZqrbdyPSoDvgAx8octcFCM0nvE.png" | relURL }})
### **TIS**
$$
TIS = (Scls + Sself + Stoken)/3                      



$$


**æ¯”å¦‚æ‹¿ visual æ¨¡æ€æ¥ä¸¾ä¾‹ï¼š**


## **VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models**
![image]({{ "images/PzhNbrL9SocCajxN5CncqrLDnJe.png" | relURL }})
---
### **Observations**
1. **In the visual encoding stage, the visual encoder attends to locally significant tokens in the shallow layers, focusing on fine-grained local details, while at deeper layers, it gradually shift its focus to a highly condensed set of tokens that encapsulate broader global context;ï¼ˆvisual encoderé‡Œé¢éšç€å±‚æ•°å¢å¤šï¼Œæ³¨æ„åŠ›å‘ˆç°çš„å˜åŒ–è¶‹åŠ¿,ä¸‹é¢ä¸¤ä¸ªå›¾æˆ‘è‡ªå·±å¯è§†åŒ–äº†ä¸€ä¸‹ï¼Œå¯ä»¥çœ‹åˆ°attention focusç”±å¹¿æ³›åˆ°é›†ä¸­ï¼Œä½†é—®é¢˜æ˜¯é›†ä¸­çš„éƒ¨åˆ†å¹¶ä¸å®Œå…¨æ˜¯æˆ‘ä»¬äººçœ¼æ‰€æ™®éè®¤ä¸ºçš„focusï¼‰**


1. **In the LLM decoding stage, early layers exhibit strong positional bias toward visual tokens appearing later in the sequence, neglecting their semantic relevance; as the layers deepen, cross-modal interactions begin to emerge, and output token probabilities typically converge in the mid-to-late layers where visual information is more effectively integrated into the language stream.ï¼ˆå’ŒFsterVLMé‡Œé¢æåˆ°çš„attention dispersionæ˜¯ä¸€ä¸ªä¸œè¥¿ï¼‰**

### **Current methods limitations**
**å¯ä»¥çœ‹åˆ°è¿™é‡Œç”¨äº†è¿™æ ·ä¸€å¼ å›¾ç‰‡å’Œqueryæ¥disableä¹‹å‰çš„å‡ ç§ä¸»æµæ–¹æ³•**

![image]({{ "images/ZtWWbX6aHo8m1Cx9xQdcZulHnEe.png" | relURL }})

**ç„¶ååšäº†3ä¸ªstudyï¼šå„ç§å¯è§†åŒ–å»æ¢ç©¶LLMå¯¹äºtextualå’Œvisualä¿¡æ¯çš„å¤„ç†éšç€å±‚æ•°å˜åŒ–çš„æ”¹å˜æƒ…å†µ**
![image]({{ "images/OQvkb3nnuopOgrxweM2cPr0sn5I.png" | relURL }})
**å·¦è¾¹çš„å›¾å…¶å®å°±æ˜¯attention shiftçš„å¯è§†åŒ–ï¼ˆä½ç½®ç¼–ç çš„å½±å“ï¼‰ï¼Œéšç€LLM layerså¢å¤šï¼Œè¿™ç§ç°è±¡é€æ¸diminish**

**å³è¾¹çš„å›¾ï¼šWe observe that the middle LLM layers are primarily responsible for interacting with the visual tokens, whereas the early and deep layers focus predominantly on processing textual information.å°±æ˜¯LLMçš„ä¸­é—´çš„é‚£äº›layersä¼šæ›´å€¾å‘äºç»“åˆvisualä¿¡æ¯èšåˆå¤„ç†ï¼Œç„¶è€Œshallow/deep layeréƒ½ä¼šfocusæ›´å¤šåœ¨textualä¿¡æ¯ä¸Š**
![image]({{ "images/IO9vbikGqoUEXwxNisucXMZHnG0.png" | relURL }})
**We observe that in more challenging open-ended tasks like GQA, the next-token predictions stabilize around LLM layer 20, whereas in simpler yes/no tasks such as POPE, the predictions converge earlier, around LLM layer 16.**

**åœ¨è¿™ä¸€éƒ¨åˆ†å¾—å‡ºçš„ç»“è®ºå°±æ˜¯ï¼šLLMçš„early layerså¹¶ä¸æ˜¯æœ€é€‚åˆpruningçš„å±‚æ•°ä½ç½®ï¼Œå› ä¸º**
1. ** positional biasï¼›**

1. ** limited engagement of visual content**

**è€Œmiddle layersæ‰æ˜¯æœ€å¥½çš„ï¼Œå› ä¸ºï¼š**
1. **better preserves critical cross-modal interactions**

1. **minimizes disruption to model predictionsï¼ˆå¤ªæ·±å±‚èµ·ä¸åˆ°å¤ªå¥½çš„pruningæ•ˆæœè€Œä¸”å¯èƒ½ä¼šæœ‰disruptï¼‰**

### **Methods**
![image]({{ "images/MtSRbnAfHoWKSexJlXGctCgDn4e.png" | relURL }})
#### **Reducing Visual Redundancy via Complementary Global and Local Scans**
##### **Global Scan**
**å› ä¸ºvisual encoderçš„æœ€åä¸€å±‚ï¼ˆæˆ–è€…å€’æ•°ç¬¬äºŒå±‚ï¼‰æ˜¯global contentï¼Œæ‰€ä»¥é‡‡ç”¨è·Ÿä¹‹å‰å·¥ä½œç±»ä¼¼çš„æ–¹æ³•CLS attentionçš„æ–¹å¼æ¥é€‰æ‹© global tokens ï¼ˆgï¼‰**
##### **Local Scan**
**å› ä¸ºå‰é¢å·²ç»åˆ†æè¿‡äº†ç›´æ¥ä½¿ç”¨CLS attentionè·å–å…¨å±€çš„ä¿¡æ¯ä¼šä½¿å¾—ä¸€äº›é‡è¦çš„ç»†èŠ‚ä¿¡æ¯é—æ¼æ‰ï¼Œæ‰€ä»¥è¿˜è¦åœ¨visual encoderçš„æµ…å±‚æ¥è·å–ä¸€äº›finerçš„ç»†èŠ‚ä¿¡æ¯ï¼Œå…·ä½“é€‰æ‹©æ–¹å¼æ˜¯æŒ‰ç…§windowså»åšé€‰æ‹©ï¼ˆlï¼‰ï¼Œgå’Œlçš„æ•°é‡è¢«æ§åˆ¶ä¸ºä¸€æ ·**
##### **Token Merging**
**æŠŠæœªè¢«é€‰æ‹©çš„é‚£äº›tokenä¸è¢«é€‰æ‹©çš„tokenåšå†…ç§¯ï¼Œå¯¹æ¯ä¸ªunselected tokené€‰æ‹©æœ€ç›¸ä¼¼çš„selected tokenå¹¶ä¸”åšaverage mergeå¾—åˆ°æœ€ç»ˆçš„merged representationï¼ˆæ„Ÿè§‰è¿™ä¸ªåœ°æ–¹åœ¨archå±‚é¢å¯ä»¥è€ƒè™‘é€‚å½“çš„ä¼˜åŒ–ï¼‰**
#### **Reducing Textual Irrelevance via Middle Layer Pruning**
**Further refine the token set based on their relevance to the text query**

### **Experiment Scores**


## **Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping**
![image]({{ "images/IjGpbXnSroa5uoxmbiAcdZzIn9g.png" | relURL }})
---

## **Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight**
![image]({{ "images/DI3hbanZloB0KCxy4iccxpavnEe.png" | relURL }})
---
### **Brief Introduction **
**ç”¨æ›´å°‘çš„visual tokensè®­ç»ƒé€šå¸¸æ„å‘³ç€perfomanceçš„ä¸‹é™ï¼Œé‚£ä¹ˆæœ‰æ²¡æœ‰ä¸€ç§æ–¹å¼èƒ½å¤Ÿè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç”¨æ›´å°‘çš„visual tokenså»åŒ…å«æ›´å¤šçš„ä¿¡æ¯ï¼Œä¸”èƒ½ä¸å—input resolutionçš„å½±å“ï¼Œä»è€Œå®ç°æ›´efficientçš„pre-trainingï¼Ÿ**

**CoS(chain of sight)å°±æ˜¯è¿™æ ·ä¸€ä¸ªæ–¹å¼ï¼Œè¿™æ˜¯ä¸€ä¸ªvision-language bridgeçš„æ¨¡å—ï¼Œæ•´ä½“çš„æ€è·¯æœ‰ç‚¹ç±»ä¼¼ä¹‹å‰çš„PerceiveræŠ‘æˆ–æ˜¯Q-formerï¼Œä½†è¿˜æœ‰ä¸€ä¸ªç‰¹ç‚¹æ˜¯å¯¹é¢„è®­ç»ƒå’Œå¾®è°ƒéƒ¨åˆ†ä½¿ç”¨çš„tokenæœ‰å¾ˆå¤§å·®åˆ«ï¼Œåè€…ä½¿ç”¨æ›´å¤šæ›´fineçš„tokensï¼Œä»¥è·å–æ›´finerçš„visionä¿¡æ¯ï¼Œä»è€Œå¼¥è¡¥perfomanceå¯èƒ½çš„æ‰ç‚¹**
![image]({{ "images/HSSVbssrXodcvmxT2z4ciYR5n8d.png" | relURL }})
```plaintext
The core mechanism is our multi-scale visual resampler, which produces visual tokens 
of multiple visual scales. Inspired by the classical concept of multi-scale feature hierarchy in visual 
understanding [105, 41, 32, 106, 82, 52], we partition the visual features produced by the visual 
backbone using windows of multiple sizes. For each window size, a visual resampler is implemented 
to produce a specified number of visual tokens per window. Subsequently, the visual tokens from 
various window sizes are gathered and linked in a global-to-local manner, forming a chain of reasoning 
steps from coarse views gradually to fine-grained perspectives.
On top of this, we propose a post-pretrain token scaling strategy, which compounds the elements of 
input resolution and window size manipulation to enable a significant escalation in the token count 
for our Chain-of-Sight, reaching up to 16Ã— increase during fine-tuning. Such adaptability allows for 
the fine-tuning of the model with a flexible granularity or complexity as required, without the the 
necessity for an additional pre-training phase.


å›¾åƒ â†’ ViTè§†è§‰ç¼–ç å™¨ â†’ Multi-Scale Visual Resampler â†’ å°‘é‡ token è¾“å…¥è¯­è¨€æ¨¡å‹ â†’ é¢„è®­ç»ƒ  
                                               â†“  
                      ï¼ˆå¾®è°ƒæ—¶å†æ”¾å¤§ token æ•°é‡ï¼‰ â†’ å¤š token è¾“å…¥è¯­è¨€æ¨¡å‹ â†’ ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒ

```
### **Methods Delineate**
#### **Multi-scale visual Resamplers**


#### **Post-Pretrain Token Scaling**
![image]({{ "images/BPBibeDZZokNk6xe0LycsqwXnuc.png" | relURL }})
1. **åœ¨é¢„è®­ç»ƒä¸­åªç”¨å°‘é‡è§†è§‰ tokenï¼ˆå¦‚ 32/80ï¼‰ï¼Œå¤§å¹…åŠ é€Ÿè®­ç»ƒï¼›**

1. **å¾®è°ƒé˜¶æ®µå†å°† token æ•°æ‰©å¤§ï¼ˆé€šè¿‡è°ƒæ•´è¾“å…¥åˆ†è¾¨ç‡ + å‡å°çª—å£ sizeï¼‰ï¼›**

1. **æå‡º compound scalingï¼šresolution scaling Ã— window scalingï¼Œæœ€å¤šå¯å°† token æ•°æ‰©å¤§ 16 å€ï¼ˆå¦‚ 32 â†’ 512ï¼‰ï¼›**

```plaintext
Step 1: é¢„è®­ç»ƒé˜¶æ®µ
    å›¾åƒåˆ†è¾¨ç‡ä½ï¼ˆ224Ã—224ï¼‰
    window size ç²—ï¼ˆ16ã€8ï¼‰
    æ¯ä¸ª window åˆ†é…å°‘é‡ query
    â†’ å¾—åˆ°å°‘é‡ï¼ˆå¦‚ 32ã€80ï¼‰è§†è§‰ token
    â†’ å¿«é€Ÿé¢„è®­ç»ƒ

Step 2: å¾®è°ƒé˜¶æ®µ
    æé«˜åˆ†è¾¨ç‡ï¼ˆå¦‚ 448Ã—448ï¼‰
    å¼•å…¥æ›´å°çš„ window sizeï¼ˆ8ã€4ã€2ï¼‰
    æ¯ä¸ª window åˆ†é…æ›´å¤š query
    â†’ å¾—åˆ°æ›´å¤šï¼ˆå¦‚ 336ã€528ã€1296ï¼‰è§†è§‰ token
    â†’ ç»†ç²’åº¦ç†è§£å›¾åƒï¼Œæé«˜ä»»åŠ¡æ€§èƒ½
```
## **HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers**
---
### Background & Target
_**HeatViT**__: _While vision transformers (ViTs) have continuously achieved new milestones in the field of computer vision, their sophisticated network architectures with high computation and memory costs have impeded their deployment on resource-limited edge devices

we propose a hardware-efficient image-adaptive token pruning framework called HeatViT for efficient yet accurate ViT acceleration on embedded FPGAs.

_**SPViT: **_high computation and memory cost

a dynamic attention-based multi-head token selector, which is a lightweight module for adaptive instance-wise token selection. We further introduce a soft pruning technique, which integrates the less informative tokens chosen by the selector module into a package token rather than discarding them completely

project: https://github.com/PeiyanFlying/SPViT
### Algorithm
#### _Head-Evaluation Multi-Head __Token Classifier_
_**SPViT:**_

æ•´ä½“å‘¢å°±æ˜¯é‡‡ç”¨ä¸€ä¸ªå¯å­¦ä¹ çš„ç½‘ç»œï¼ˆä¸»è¦å°±æ˜¯å‡ å±‚å¾ˆå°çš„MLPï¼Œå®é™…è®¡ç®—é‡ä¸è¶³ViTçš„1%ï¼‰ï¼Œç”¨è¿™ä¸ªç½‘ç»œå»å­¦ä¹ keep/pruneçš„è§„åˆ™ã€æ¨¡å¼ï¼Œç®—æ³•æ€è·¯ä¸Šå°±æ˜¯æ¯ä¸ªheadå…³æ³¨çš„ç‰¹å¾å’Œéƒ¨åˆ†æ˜¯ä¸ä¸€æ ·çš„ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ªheadæœ¬èº«å¯¹äºæ‰€æœ‰tokenæ˜¯è‡ªå¸¦ä¸€å®šæ³¨æ„åŠ›çš„ï¼Œå¯ä»¥å­¦ä¹ åˆ°head scoreé‚£ä¹ˆè‡ªç„¶å°±ä¼šæƒ³åˆ°åœ¨æœ€åconcatçš„æ—¶å€™é‡‡ç”¨åŠ æƒå¹³å‡çš„æ–¹å¼ï¼Œä¹Ÿå°±æ˜¯ç”¨ $$
head\ score \times token \ score \ for each \ head
$$
æ¥è¡¨ç¤º
![image]({{ "images/UeyTbGK3WooyitxO2oMcFFf1nCd.png" | relURL }})
1. $$
MLP_1:LayerNorm \rightarrow Linear(d,d/2) \rightarrow GELU
$$

1. $$
MLP_2:Linear(d,d/2) \rightarrow GELU \rightarrow Linear(d/2,d/4) \rightarrow GELU \rightarrow Linear(d/4,2)
$$

1. $$
MLP_3:Linear(H,H/2) \rightarrow GELU \rightarrow Linear(H/2,H) \rightarrow Sigmoid
$$

1. $$
f_i^{local}=MLP_1(x_i) \in \mathbb{R}^{N \times d/2} 
$$

1. $$
f_i^global=AvgPool(MLP_1(x_i),D) \in \mathbb{R}^{1 \times d/2} 
$$

1. $$
f_i=[f_i^local,f_i^global] \in \mathbb{R}^{N \times d}
$$

1. $$
t_i=Softmax(MLP_2(f_i)) \in \mathbb{R}^{N \times 2}
$$

1. $$
\bar{X}=AvgPool(X) \in \mathbb{R}^{N \times H}
$$

1. $$
A=MLP_3(\bar{X})
$$

1. $$
\tilde{T} = \frac{\sum_{i=1}^{H} t_i \ast a_i}{\sum_{i=1}^{H} a_i} \in \mathbb{R}^{N \times 2}
$$

1. $$
D=GumbelSoftmax( \tilde{T}) \in \{0,1\}^N
$$

1-3æ˜¯MLPçš„declarationï¼Œ4-7æ˜¯token score branchï¼Œ8-10æ˜¯head score branch

MLPéƒ½ç”¨é™ç»´ï¼Œå› ä¸ºä¸æ˜¯è¦æå–æ›´è¯¦ç»†æ›´å¤æ‚çš„ç‰¹å¾ä¿¡æ¯ï¼Œç›®æ ‡æ˜¯ç”Ÿæˆä¸€ä¸ªç®€å•ã€æ¸…æ™°ã€ä½ç»´çš„ç­–ç•¥ scoreï¼Œè€Œä¸æ˜¯ä¿ç•™åŸå§‹è¯­ä¹‰ã€‚head scoreæœ€åactivationç”¨äº†Sigmoidï¼Œä¸ºäº†ä¾¿äºåŠ æƒ

token scoreçš„éƒ¨åˆ†ï¼Œä¸ºä»€ä¹ˆä¿ç•™åˆ°2ç»´ï¼Ÿè¿™ä¸¤ç»´åˆ†åˆ«ç”¨äºè¡¨ç¤ºkeep/pruneçš„æ¦‚ç‡ï¼Œè¿™ä¹Ÿæ˜¯è¿™ä¸ªç½‘ç»œæœ€æ ¸å¿ƒæœŸæœ›å­¦ä¹ åˆ°çš„ç‰¹å¾ï¼Œå¦ä¸€æ–¹é¢åç»­ä¼šç”¨åˆ°Gumbel Softmaxï¼Œä¸ºäº†åå‘ä¼ æ’­å­¦ä¹ ï¼Œè€Œä¸æ˜¯ç›´æ¥ç”¨argmaxç­‰æ¥å†³å®šæ˜¯pruneè¿˜æ˜¯keepï¼ŒGumbel Softmax æ˜¯è§£å†³â€œå¯å¾®åˆ†ç¦»æ•£é€‰æ‹©â€çš„æ ‡å‡†åšæ³•ï¼ŒSPViT é€šè¿‡Gumbel Softmaxå®ç°äº† token pruning çš„ end-to-end è®­ç»ƒå’Œéƒ¨ç½²é—­ç¯ã€‚

_**HeatViT:**_

#### Token Packaging Technique
åšè¿™éƒ¨åˆ†packagerçš„åŸå› å¾ˆç®€å•ï¼š1. ç›´æ¥pruneçš„è¯ä»ç„¶æ˜¯ä¼šä¸¢æ‰ä¸å°‘ä¿¡æ¯ï¼Œæ‰€ä»¥æƒ³èšåˆä¸€ä¸‹ï¼Œä¸ç›´æ¥ä¸¢æ‰ä¿¡æ¯ï¼›2. å¦å¤–åœ¨transformeré‡Œï¼Œearlier blockå—åˆ°è¿™ç§infoçš„ä¸¢å¤±å½±å“ä¼šæ›´æ˜¾è‘—ï¼›3. æ­¤å¤–åœ¨â€œInstance localization for self-supervised detection pre-trainingâ€è¿™ç¯‡æ–‡ç« æåˆ°äº†ï¼Œbackgroundä¿¡æ¯çš„è¿‡å¤šå‰”é™¤ä¼šå¯¼è‡´self-attentionæå–å…³é”®ç‰¹å¾çš„èƒ½åŠ›é™ä½ï¼›4. ç»“åˆè¿™äº›è¢«pruneæ‰çš„tokensçš„ä¿¡æ¯åˆ°ä¸€ä¸ªç»Ÿä¸€çš„package tokené‡Œï¼Œä¿æœ‰ä¸€å®šçš„è¯­ä¹‰ä¿¡æ¯


### Hardware 
ç¡¬ä»¶ä¸»è¦åšäº†å‡ éƒ¨åˆ†ä¼˜åŒ–ï¼š1. æ§åˆ¶æµçš„è®¾è®¡å»å°½å¯èƒ½å¤šçš„å¤ç”¨ViTå·²æœ‰çš„backboneéƒ¨ä»¶ï¼›2. å¹¶è¡ŒåŒ–çš„ä¼˜åŒ–ï¼Œå¯¹äºGEMMæ”¯æŒå¤šå¤´çš„å¹¶è¡Œå¤„ç†ï¼›3. ä¼˜åŒ–éçº¿æ€§è¿ç®—æ“ä½œï¼›4. LayerNormæ˜¯åœ¨CPUä¸Šåšçš„
![image]({{ "images/SY4EbGvNooVYiHxdTXNcVt8LnVd.png" | relURL }})

### Limitations / Weakness / Further research 
HeatViTæ˜¯ç”¨å¯å­¦ä¹ çš„å°å‹ç½‘ç»œæ¥é€‰æ‹©prune/keepçš„tokenï¼Œä½†ç¼ºç‚¹å°±æ˜¯å¯¹äºç‰¹å®šçš„å›¾ç‰‡ï¼Œåœ¨inferenceçš„æ—¶å€™æ•´ä¸ªç½‘ç»œçš„focusæ˜¯å›ºå®šçš„ï¼Œä½†åœ¨VLMä¸­å®é™…çš„focuså¿…ç„¶ç¦»ä¸å¼€promptçš„è¯­è¨€tokenéƒ¨åˆ†


## **ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention**
---
### Background & Target
we propose a first-of-its-kind algorithm-hardware co-designed framework, dubbed VITALITY, for boosting the inference efficiency of ViTs. Unlike sparsity-based Transformer accelerators for NLP, VITALITY unifies both low-rank and sparse components of the attention in ViTs.
### Algorithm
#### Mean-centered K


#### Taylor-softmax attention
ViTalityæƒ³ç”¨çº¿æ€§æ³¨æ„åŠ›ï¼Œé€‰æ‹©é‡‡ç”¨tayloræ¥è¿‘ä¼¼exp()ï¼Œç†ç”±å¾ˆç®€å•ï¼Œè¿›è¡Œäº†mean-centeråçš„æ³¨æ„åŠ›åˆ†æ•°å¤§å¤šé›†ä¸­åœ¨[-1,1]ä¹‹é—´ï¼Œè€Œåœ¨0é™„è¿‘é‡‡ç”¨Tayloræ¥åšè¿‘ä¼¼exp()æ˜¯ä¸é”™çš„é€‰æ‹©ï¼Œæ‰€ä»¥æ ¸å¿ƒçš„æ€è·¯å°±æ˜¯ç”¨taylorå±•å¼€çš„ä¸€é˜¶é¡¹æ¥è¿‘ä¼¼æ³¨æ„åŠ›åˆ†æ•°å¤„äº[-1,1]ä¹‹é—´çš„weak connectionéƒ¨åˆ†ï¼Œé‚£è‡ªç„¶ä¼šæƒ³åˆ°ç”¨é«˜é˜¶é¡¹æ¥è¿‘ä¼¼strong connectionéƒ¨åˆ†ï¼Œä½†é—®é¢˜åˆå‡ºç°äº†ï¼Œé«˜é˜¶çš„è®¡ç®—åŠå…¶å¤æ‚ï¼Œå¾ˆå¯èƒ½å°±offsetæ‰linear attentionå¸¦æ¥çš„ä¼˜åŠ¿äº†ï¼Œæ‰€ä»¥éœ€è¦æ‰¾æ–¹æ³•æ›¿ä»£é«˜é˜¶/strong connectionçš„è®¡ç®—ï¼ŒViTalityé€‰æ‹©çš„æ–¹æ³•æ˜¯ç”¨Sangerï¼ˆè°ƒæ•´thresholdï¼‰æ¥ä»¥Sparse attentionçš„æ–¹å¼è¿‘ä¼¼å¤„ç†strong connectionã€‚æ‰€ä»¥ä¸€å¥è¯æ€»ç»“ViTalityç®—æ³•å±‚é¢å°±æ˜¯ï¼šç”¨unifiedçš„Linear Taylor Attentionä»¥åŠSparse Attentionæ¥æ›¿ä»£ä¼ ç»Ÿçš„Softmax Attentionæ¥å‡å°è®¡ç®—å¤æ‚åº¦ $$
O(n^2) \rightarrow O(n)
$$
ã€‚
![image]({{ "images/FxezbG1ZAoCDCwxGO0DcsceKn3c.png" | relURL }})
å¦å¤–æ³¨æ„ï¼šViTalityåœ¨è®­ç»ƒé˜¶æ®µé‡‡ç”¨çš„æ˜¯Linear+Sparseçš„å½¢å¼åšè®­ç»ƒï¼Œåè€…èƒ½èµ·åˆ°æ­£åˆ™åŒ–çš„ä½œç”¨ã€‚ä½†æ˜¯åœ¨æ¨ç†çš„æ—¶å€™ä»…ä»…è€ƒè™‘Linearçš„éƒ¨åˆ†ï¼Œè€Œå¿½è§†æ‰äº†Sparseéƒ¨åˆ†ï¼Œå®šç„¶ä¼šæœ‰äº›è®¸çš„æ‰ç‚¹ä½†æ˜¯è€ƒè™‘åˆ°å…¶ä»–å› ç´ å¯ä»¥æ¥å—ã€‚
### Hardware
micro-architecture
![image]({{ "images/N6ZsbmjUFoLOPLxe5BgcY1s3nqg.png" | relURL }})
####  å¤šå—å¼è®¾è®¡ï¼ˆChunk-based Designï¼‰
- ä¸ç”¨ä¸€ä¸ªå¯é‡æ„å¤„ç†é˜µåˆ—å»è·‘æ‰€æœ‰æ“ä½œï¼ˆè¿™æ ·å¼€é”€å¤§ï¼‰ï¼Œè€Œæ˜¯åˆ†æˆå¤šå—ï¼š
	- å¤§é˜µåˆ—ï¼ˆSA-Generalï¼‰ï¼šè´Ÿè´£å¤§è§„æ¨¡çŸ©é˜µä¹˜æ³•ï¼Œæ¯”å¦‚ `QG`ã€`KÌ‚^T V`ã€‚
		- å°é˜µåˆ—ï¼ˆSA-Diagï¼‰ï¼šè´Ÿè´£å¯¹è§’çŸ©é˜µä¹˜æ³•ï¼ˆå¦‚ `Q kÌ‚_sum^T`ï¼‰ï¼Œä¹˜æ³•é‡å°å¾—å¤šï¼Œåªç”¨ä¸€åˆ—PEã€‚
		- ç´¯åŠ å™¨é˜µåˆ—ï¼ˆAccumulator Arrayï¼‰ï¼šåˆ—æ–¹å‘æ±‚å’Œï¼Œç”¨äº `kÌ‚_sum` å’Œ `v_sum` ç­‰ã€‚
		- é™¤æ³•é˜µåˆ—ï¼ˆDivider Arrayï¼‰ï¼šä¸¤ç§æ¨¡å¼â€”â€”å•é™¤æ•°ï¼ˆå‡å€¼åŒ–ï¼‰ & å¤šé™¤æ•°ï¼ˆTaylor åˆ†å­åˆ†æ¯ç›¸é™¤ï¼‰ã€‚
		- åŠ æ³•é˜µåˆ—ï¼ˆAdder Arrayï¼‰ï¼šå…ƒç´ çº§åŠ /å‡ï¼ˆå¦‚å‡å€¼ä¸­å¿ƒåŒ–ã€Taylor åˆ†å­åˆ†æ¯åŠ æ³•ï¼‰ã€‚
	
ä¼˜åŠ¿ï¼š
- ä¸“ç”¨å•å…ƒé¿å…å¤§é˜µåˆ—è·‘å°ä»»åŠ¡çš„æµªè´¹ã€‚

- å°é˜µåˆ—ã€è½»è¿ç®—å•å…ƒåŠŸè€—ä½ã€é¢ç§¯å ç”¨å°ã€‚

- ä¸åŒå—å¯å¹¶è¡Œæ‰§è¡Œï¼Œæ–¹ä¾¿æµæ°´çº¿ã€‚

---
####  å››çº§å­˜å‚¨å±‚æ¬¡ï¼ˆMemory Hierarchyï¼‰
- DRAM â†’ SRAM â†’ NoC â†’ å¯„å­˜å™¨ï¼ˆRegsï¼‰
	- DRAMï¼šå¤§å®¹é‡å­˜å‚¨
		- SRAMï¼šç‰‡ä¸Šç¼“å­˜ï¼Œå‡å°‘DRAMè®¿é—®
		- NoCï¼šç‰‡å†…ä¼ è¾“
		- Regsï¼šæ¯ä¸ªè®¡ç®—å•å…ƒå±€éƒ¨å¯„å­˜å™¨ï¼Œé…åˆ systolic array æ•°æ®å¤ç”¨
	
- æ•°æ®å¤ç”¨ä¼˜åŒ–ï¼š
	- çŸ©é˜µä¹˜æ³•æ—¶ï¼Œè®©æƒé‡æˆ–ä¸­é—´ç»“æœåœ¨PEå†…é©»ç•™ï¼ˆstationaryï¼‰ä»¥å‡å°‘è®¿å­˜ã€‚
		- `V` åœ¨è®¡ç®— `KÌ‚^T V` æ—¶é©»ç•™ï¼Œ`G` åœ¨è®¡ç®— `QG` æ—¶é©»ç•™ã€‚
	
---
#### æµæ°´çº¿åˆ›æ–°ï¼ˆIntra-Layer Pipelineï¼‰


---
#### æ•°æ®æµåˆ›æ–°ï¼ˆDown-Forward Accumulation Dataflowï¼‰
![image]({{ "images/JSx0b7dyYoGy5Mx5eBycHMj2nPg.png" | relURL }})
- å¸¸è§ä¸¤ç§æ•°æ®æµï¼š
	Output Stationaryï¼šè¾“å‡ºç•™åœ¨PEå†…ï¼ˆå†…ç´¯åŠ ï¼‰	Input Stationaryï¼šè¾“å…¥æƒé‡ç•™åœ¨PEå†…ï¼ˆè¡Œ/åˆ—ç§»åŠ¨ï¼Œå‘ä¸‹ç´¯åŠ ï¼‰
- ViTALiTy çš„é€‰æ‹©ï¼š
	- å…¨éƒ¨çŸ©é˜µä¹˜æ³•ç»Ÿä¸€ç”¨ Input Stationary + Down-forward accumulationã€‚
		- å¥½å¤„ï¼š
		- ä¸ç”¨ä¸ºä¸åŒçŸ©é˜µä¹˜æ³•åˆ‡æ¢ç´¯åŠ æ¨¡å¼ â†’ ç®€åŒ–PEè®¾è®¡ã€‚
			- é™ä½ systolic array åŠŸè€—ï¼ˆå®éªŒè¡¨æ˜ systolic array çš„èƒ½è€—å æ€»èƒ½è€—å¤§å¤´ï¼‰ã€‚
			- ä»£ä»·ï¼š
		- `G` ä¸é©»ç•™ â†’ è®¿å­˜é‡å¢åŠ ï¼Œä½†æ€»èƒ½è€—åè€Œä¸‹é™ï¼ˆå› ä¸ºçŸ©é˜µä¹˜æ³•èƒ½è€—å‡å°‘æ›´å¤šï¼‰ã€‚
		
### Limitations / Weakness / Further research
åœ¨å¤„ç†å¼±è¿æ¥çš„æ—¶å€™è®¤ä¸º[-1,1]æ˜¯weakï¼Œå¹¶ç”¨ä¸€é˜¶æ¥è¿‘ä¼¼ï¼Œæœ‰ä¸€ä¸ªé—®é¢˜æ˜¯æ³¨æ„åŠ›åˆ†å¸ƒä¸è§å¾—éƒ½æ˜¯å¤§éƒ¨åˆ†ä¸ºå¼±è¿æ¥çš„ï¼Œå¯èƒ½éšç€è¾“å…¥patchæˆ–è€…æ˜¯ä¸åŒheadç­‰ç­‰çš„å˜åŒ–ï¼Œç”šè‡³ä¼šå‡ºç°å¤§éƒ¨åˆ†ä¸ºå¼ºè¿æ¥ï¼Œé‚£è¿™æ—¶çš„æ‰ç‚¹å¯ä»¥é¢„æƒ³ä¼šå¾ˆä¸¥é‡ã€‚------static åˆ¤å®šçš„ä¸è¶³ã€‚

ç¨€ç–éƒ¨åˆ†åœ¨è®­ç»ƒå¾®è°ƒæœ‰ç”¨ï¼Œä½†æ˜¯åœ¨æ¨ç†çš„æ—¶å€™ç”¨ä¸åˆ°ï¼šè¯´æ˜sparse attentionå¾ˆå¯èƒ½åªèµ·åˆ°äº†æ­£åˆ™åŒ–ä½œç”¨ï¼Œè€Œç¼ºä¹æä¾›è¶³å¤Ÿè¯­ä¹‰ä¿¡æ¯çš„èƒ½åŠ›---å¯èƒ½åœ¨ä¸€äº›å¼ºå±€éƒ¨ç›¸å…³çš„ä»»åŠ¡ä¼šè¡¨ç°æ¯”è¾ƒå·®




## **ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design**
---
### Background & Target
ViTs have a relatively fixed number of input tokens, whose attention maps can be pruned by up to 90% even with fixed sparse patterns, without severely hurting the model accuracy
### Algorithm
ViTCoDçš„æ ¸å¿ƒç®—æ³•æ˜¯split and conquer & auto encoder
#### _ViTCoDâ€™s Split and Conquer Algorithm / hardware_
ViTCoDé‡‡ç”¨fixed maskçš„ç¨€ç–æ ¼å¼ï¼Œæ‰”è¿›å»æ‰€æœ‰æ•°æ®ï¼Œç®—å‡ºæ¯ä¸€ä¸ªæ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ$$
A
$$
å¹¶ä¸”æ±‚å¹³å‡ï¼Œæ ¹æ®è¿™ä¸ªå¹³å‡å€¼æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ$$
\bar{A}
$$
æ¥æ„å»ºä¸€ä¸ªfixed maskï¼Œå¹¶ä¸”åœ¨æ¨ç†çš„æ—¶å€™ç›´æ¥ç”¨è¿™ä¸ªmaskå›ºå®šåŒ–ç¨€ç–æ¨¡å¼


#### _ViTCoD Learnable Auto-encoder Module_


### Hardware
#### Why
- ViTCoD çš„ S&C è®©æ³¨æ„åŠ›å˜æˆå›ºå®šä¸”ç»“æ„åŒ–çš„ç¨€ç–ï¼Œå†é…åˆ AE å‹ç¼© Q/Kï¼Œç»™ç¡¬ä»¶æä¾›äº†ä¸¤ç±»æ–°æœºä¼šï¼š
 â‘  å›ºå®šç¨€ç–å›¾ä¸å†éœ€è¦åœ¨çº¿é¢„æµ‹ã€æ§åˆ¶æ›´ç®€å•ï¼›â‘¡ Q/K å¯è¢«å‹ç¼©å‡å°‘æ•°æ®æ¬è¿ã€‚ç„¶è€Œè§†è§‰é‡Œçš„ç¨€ç–æ³¨æ„åŠ›å¸¸æ²¿å¯¹è§’çº¿é›†ä¸­ï¼Œä¼šé€ æˆæ•°æ®é‡ç”¨å·®ã€PE åˆ©ç”¨ç‡ä½ä¸”å¸¦å®½å—é™ï¼Œå› æ­¤éœ€è¦â€œç®—æ³•+åŠ é€Ÿå™¨â€ååŒè®¾è®¡ï¼ˆæ–‡ä¸­ roofline åˆ†ææ˜ç¡®ï¼šä»…ç¨€ç–è¿˜ä¼šæ›´å—å¸¦å®½é™åˆ¶ï¼Œå¿…é¡»å†é™é€šä¿¡ï¼‰ã€‚

#### Dataflowï¼šS-stationary vs K-stationary
- è®ºæ–‡æ¯”è¾ƒäº†ä¸¤ç§åš SDDMM(QÂ·Káµ€) çš„æ•°æ®æµï¼š
S-stationaryï¼šæŠŠæ³¨æ„åŠ›å¾—åˆ†â€œç©ºé—´æ˜ å°„â€åˆ° PE é˜µåˆ—ï¼Œæ¯ä¸ª PE ç®—ä¸€ä¸ªåˆ†æ•°â€”â€”è¿™å¯¹ç¨€ç–ä¸å‹å¥½ï¼šPE åˆ©ç”¨ç‡ä½ã€æ§åˆ¶/é‡æ„å¼€é”€å¤§ã€è¿˜è¦åœ¨ PE å¯„å­˜å™¨é‡Œå­˜å¤§é‡ä¸­é—´éƒ¨åˆ†å’Œåš intra-PE ç´¯åŠ ã€‚ä»£è¡¨ä½œ Sanger ç”¨çš„å°±æ˜¯å®ƒã€‚
K-stationaryï¼šæŒ‰åˆ—ç”Ÿæˆæ³¨æ„åŠ›åˆ†æ•°ï¼ŒK å‘é‡é‡ç”¨å……åˆ†ã€ä¸­é—´ç¼“å†²å°ã€ä¸”åªæŒ‰ç¨€ç–ç´¢å¼•é…å¯¹ Q/K åšä¹˜æ³•ï¼Œå¤©ç”Ÿæ›´é€‚åˆç¨€ç–ã€‚ä½†ç¼ºç‚¹æ˜¯Q è®¿é—®æ›´é¢‘ç¹â€”â€”è®ºæ–‡è¯´è¿™ä¸ªç¼ºç‚¹ç”± AE å‹ç¼©æ¥ç¼“è§£ï¼ˆå°‘æ¬ Qï¼‰ã€‚ç»“è®ºï¼šé€‰ K-stationaryã€‚

#### Two-pronged
- åŠ é€Ÿå™¨ç”±ä¸¤å¥—ç‹¬ç«‹è®¡ç®—å¼•æ“ç»„æˆï¼š
Denser Engine è´Ÿè´£ SDDMM çš„â€œå–æ ·è‡´å¯†â€éƒ¨åˆ†å’Œåç»­ SpMM çš„ SÂ·Vï¼›
Sparser Engine è´Ÿè´£å‰©ä½™çš„é«˜åº¦ç¨€ç–éƒ¨åˆ†ï¼›
 ä¸¤è·¯æœ‰ç‹¬ç«‹è¾“å‡ºç¼“å†²å¹¶è¡Œå†™å›ï¼›èŠ¯ç‰‡å†…è¿˜é›†æˆ Encoder/Decoder å¼•æ“å»é…åˆ AEï¼Œå…ˆå‹å†æ¬ã€åˆ°é˜µåˆ—å‰å†è§£ç ã€‚

#### Denser Engine
- åŠ¨æ€ PE èµ„æºåˆ’åˆ†ï¼šä¸åŒå±‚/å¤´çš„å…¨å±€ token æ•°ä¸åŒï¼Œåˆ©ç”¨å·²çŸ¥çš„å›ºå®š maské¢„ä¼°å·¥ä½œé‡ï¼ŒæŠŠ PE/MAC æŒ‰æ¯”ä¾‹åœ¨ Denser/Sparser ä¹‹é—´åˆ†é…ã€‚

- å¹¶è¡Œä¸åˆ‡ç‰‡ï¼šå„æ³¨æ„åŠ›å¤´å¹¶è¡Œï¼Œä½†å•è¡Œ PE çº¿ä¸è¶³ä»¥ä¸€æ‹å®Œæˆ QÂ·Káµ€ï¼Œå› æ­¤åšç»†ç²’åº¦åˆ‡ç‰‡å¹¶è®¾è®¡æ—¶ç©ºæ˜ å°„ï¼š
	- SDDMM(QÂ·Káµ€)ï¼šé‡‡ç”¨ K-stationaryï¼Œåœ¨ç‰¹å¾ç»´å¯¹ Q/K åˆ‡ç‰‡å¹¶ç©ºé—´æ˜ å°„åˆ° PEsï¼Œæ—¶é—´ä¸Šè®©åŒä¸€ä¸ª Kä¾æ¬¡ä¸ç›¸å…³çš„ Q ç›¸ä¹˜ï¼Œå¹¶åœ¨ inter-PE æ–¹å‘åšéƒ¨åˆ†å’Œç´¯åŠ ï¼ˆæŒ‰åˆ—ç”Ÿæˆæ³¨æ„åŠ›ï¼‰ã€‚
		- SpMM(SÂ·V)ï¼šè½¬ä¸º output-stationaryï¼Œåœ¨token ç»´åˆ‡ç‰‡å¹¶ç©ºé—´æ˜ å°„ï¼Œæ—¶é—´ä¸Šæ²¿ç‰¹å¾ç»´ç´¯åŠ intra-PE éƒ¨åˆ†å’Œï¼Œå‡å°‘å¯¹æ³¨æ„åŠ›å›¾çš„åå¤åŠ è½½ã€æ˜¾è‘—é™ä½ on-chip ç¼“å†²å‹åŠ›ã€‚ä¸¤ç§æ¨¡å¼ä¹‹é—´éœ€è¦åœ¨ PE çº¿çº§åˆ«åˆ‡æ¢â€œinter-PE â†” intra-PEâ€ç´¯åŠ ã€‚
	
#### Sparser Engine
- ç¨€ç–åº¦å¯è¾¾ >90%ï¼Œé‡‡ç”¨ CSC ç´¢å¼•æ ¼å¼é¢„å­˜éé›¶ä½ç½®ï¼ŒæŒ‰åˆ—ï¼ˆå¥‘åˆ K-stationary çš„â€œæŒ‰åˆ—äº§å‡ºâ€ï¼‰å–ç´¢å¼•ï¼Œä»…åŠ è½½æ‰€éœ€ Q/Kï¼›è®¡ç®—åªéå†éé›¶ã€‚

- Query-based Q forwardingï¼šä¸¤è·¯å¹¶è¡Œæ—¶ï¼ŒSparser ä¾§éœ€è¦çš„æŸäº› Q å¾ˆå¯èƒ½ Denser ä¾§æ­£åœ¨ç”¨ï¼Œå› æ­¤å…ˆæŸ¥ Denser çš„ Q ç¼“å†²å†å†³å®šæ˜¯å¦ä» off-chip å–ï¼ŒæŒ‰éœ€æŸ¥è¯¢é™ä½å¸¦å®½ã€‚å…¶ä½™æ—¶ç©ºæ˜ å°„ç­–ç•¥ä¸ Denser ä¸€è‡´ã€‚

- ä¸¤è·¯éƒ½å†…ç½® SoftMax å•å…ƒï¼ˆè®¡ç®—å®Œæˆå•ä¸ªåˆ†æ•°ååš expï¼‰å’Œæ¿€æ´»å•å…ƒï¼ˆReLU ç”¨é—¨æ§ï¼Œå…¶ä»–ç”¨ LUTï¼‰ã€‚

#### Encoder/Decoder ï¼ˆAE on-chipï¼‰
- ä¸ºé…åˆ AEï¼ŒèŠ¯ç‰‡ä¸Šå®ç°äº†ç‹¬ç«‹çš„ Encoder/Decoder å¼•æ“ï¼ˆæƒé‡å¾ˆå°ï¼Œå¦‚ 6Ã—3ï¼Œå¯å¸¸é©»ç‰‡ä¸Šï¼‰ã€‚Encoder åœ¨ Q/K çº¿æ€§æŠ•å½±ä¹‹åç«‹å³å¯ç”¨ï¼ŒæŠŠ Q/K å‹ç¼©åå†å†™å› off-chipï¼›Decoder åœ¨åŠ è½½å…¥é˜µåˆ—å‰æ¢å¤ç»´åº¦ã€‚ä¸¤è€…éƒ½èƒ½å’Œæ•°æ®æ¬è¿å…¨æµæ°´é‡å ï¼Œç©ºé—²æ—¶å…¶ PE çº¿å¯å¤ç”¨äºå…¶ä»–è®¡ç®—ã€‚

#### on-chip buffer and control module
- ä¸¤è·¯å¼•æ“éƒ½é…ç½®äº†ä¸“ç”¨ç¼“å†²ï¼šè¾“å‡º(OBuf)ã€æƒé‡(WBuf)ã€K/S ç¼“å†²ã€ç´¢å¼•ç¼“å†²(IdxBuf)ã€Q/V ç¼“å†²(Q/V Buf)ï¼Œå¤šç«¯å£å¹¶è¡Œè¯»å†™ä»¥å¢å¼ºå¤ç”¨ï¼›çŸ©é˜µä¹˜æ³•æ§åˆ¶å™¨å¯åœ¨è‡´å¯†/ç¨€ç–ä¸¤ç±»è´Ÿè½½é—´åˆ‡æ¢ï¼›å¹¶å¸¦ SoftMax/Activation åŠŸèƒ½å•å…ƒã€‚

#### ç¼–è¯‘ä¸å¯é‡æ„ï¼ˆAlgorithm-Hardware Interfaceï¼‰
- ç»™å®šç»è¿‡ ViTCoD è®­ç»ƒåçš„ç¨€ç– ViT å±‚ï¼Œç½‘ç»œè§£æå™¨æŠ½å–â€œå…¨å±€ token æ•°ã€ç¼“å†²å¤§å°ã€æ•°æ®æµâ€ç­‰é…ç½®ï¼Œäº¤ç»™ç¡¬ä»¶ç¼–è¯‘å™¨ç”ŸæˆæŒ‡ä»¤ï¼ŒæŒ‡å¯¼åŠ é€Ÿå™¨åœ¨ Denser/Sparser ä¹‹é—´é‡åˆ†é… on-chip å†…å­˜å’Œ PE/MACï¼Œå¹¶åœ¨ QÂ·K ä¸ SÂ·V ä¸¤é˜¶æ®µåˆ‡æ¢ inter-PE â†” intra-PE ç´¯åŠ æ¨¡å¼ã€‚ä¸€æ¬¡ç¼–è¯‘ã€å¤šæ¬¡å¤ç”¨æ‘Šè–„é‡æ„æˆæœ¬ã€‚

#### ç«¯åˆ°ç«¯æ•°æ®æµï¼ˆæ¨ç†æ—¶ï¼Œä¸€æ¬¡æ³¨æ„åŠ›çš„â€œè½åœ°ç‰ˆâ€ï¼‰
1. çº¿æ€§æŠ•å½±â†’Encoder å‹ç¼©ï¼ˆQ/Kï¼‰ï¼Œå›å†™ off-chipï¼›2) å–ä¸‹ä¸€æ­¥éœ€è¦çš„ Q/Kï¼ŒDecoder è§£å‹è¿›å…¥ç‰‡ä¸Šï¼›3) ä¾æ® reorder çš„é¡ºåºä¸å›ºå®š maskï¼ŒæŠŠå·¥ä½œæ‹†åˆ°ä¸¤è·¯ï¼š

- Denserï¼šæŒ‰ K-stationary åš QÂ·Káµ€ï¼ˆinter-PE ç´¯åŠ ï¼‰ï¼Œå†ä»¥ output-stationary åš SÂ·Vï¼ˆintra-PE ç´¯åŠ ï¼‰ï¼›

- Sparserï¼šç”¨ CSC ç´¢å¼•åªç®—éé›¶ï¼›éœ€è¦ Q æ—¶å…ˆæŸ¥è¯¢ Denser çš„ Q ç¼“å†²å†å†³å®šå¤–å–ï¼›

1. ä¸¤è·¯å„è‡ªå†™å…¥ç‹¬ç«‹è¾“å‡ºç¼“å†²å¹¶è¡Œå›å†™/åˆå¹¶ã€‚

#### key
- PE ç´¯åŠ æ¨¡å¼åˆ‡æ¢ï¼šSDDMM ç”¨ inter-PEï¼ˆè·¨ PE èšåˆåˆ—æ–¹å‘éƒ¨åˆ†å’Œï¼‰ï¼ŒSpMM ç”¨ intra-PEï¼ˆæ¯ä¸ª PE å†…èšåˆè¾“å‡ºï¼‰ï¼Œä¸¤é˜¶æ®µåœ¨åŒä¸€ PE çº¿é‡é…ç½®ï¼Œè¿™æ˜¯è®ºæ–‡å¼ºè°ƒçš„â€œä» K-stationaryï¼ˆQÂ·Kï¼‰åˆ‡åˆ° output-stationaryï¼ˆSÂ·Vï¼‰â€ã€‚

- ä¸ºä»€ä¹ˆ K-stationary èƒ½è·‘å¾—å¥½ï¼šK è¢«å……åˆ†é‡ç”¨ã€ä¸­é—´ç¼“å†²æ›´å°ã€ä¸”å¤©ç„¶åŒ¹é…â€œæŒ‰åˆ—äº§å‡º + ç¨€ç–æŒ‰åˆ—ç´¢å¼•â€çš„å®ç°ï¼›å…¶â€œQ è®¿é—®æ›´é¢‘ç¹â€çš„ç¼ºç‚¹ç”± AE æŠµæ¶ˆã€‚

- Sparser çš„ç´¢å¼•ä¸è½¬å‘ï¼šç”¨ CSC é¢„å­˜éé›¶åˆ—ç´¢å¼•ï¼ˆé…åˆæŒ‰åˆ—äº§å‡ºï¼‰ï¼Œå¹¶ç”¨ Query-based Q forwarding åœ¨ä¸¤è·¯é—´å…±äº« Qï¼Œå‡å°‘ off-chip è®¿é—®ã€‚

> ç¡¬ä»¶å¹³å°å‚æ•°ï¼ˆè®ºæ–‡å®ç°ï¼‰ï¼šé¢ç§¯çº¦ **3 mmÂ²**ï¼ŒDDR4-2400 å¸¦å®½ **76.8 GB/s**ï¼ŒåŠŸè€— **323.9 mW@500 MHz**ï¼Œç‰‡ä¸Š **320 KB SRAM**ã€‚
### Limitations / Weakness / Further research
---
1. **å±€é™æ€§åˆ†æ**

(1) æ–¹æ³•å±‚é¢ï¼ˆS&C + AEï¼‰
- **ä¾èµ–å›ºå®š Mask**
	- SCï¼ˆSplit & Conquerï¼‰çš„ reorder + prune ä¾èµ–ä¸€ä¸ªåœ¨ fine-tuning é˜¶æ®µå­¦åˆ°çš„å›ºå®šç¨€ç–æ¨¡å¼ã€‚
		- å¯¹äºè¾“å…¥åˆ†å¸ƒå˜åŒ–å¤§æˆ– domain shift æ˜æ˜¾çš„ä»»åŠ¡ï¼Œå›ºå®š Mask å¯èƒ½å¯¼è‡´æ€§èƒ½é€€åŒ–ã€‚
		- ä¾‹å¦‚åœºæ™¯å˜åŒ–å‰§çƒˆï¼ˆä¸åŒç±»åˆ«/å¸ƒå±€çš„å›¾ç‰‡ï¼‰æ—¶ï¼Œå›ºå®šç¨€ç–æ¨¡å¼å¯èƒ½é”™è¿‡å…³é”®ä¿¡æ¯ã€‚
	
- **æ— æ³•åŠ¨æ€é€‚é…æ³¨æ„åŠ›æ¨¡å¼**
	- ä¸€æ—¦ mask å›ºå®šï¼Œæ¨ç†é˜¶æ®µä¸å†æ ¹æ®è¾“å…¥å›¾ç‰‡åŠ¨æ€ç”Ÿæˆç¨€ç–æ¨¡å¼ï¼Œè¿™åœ¨ä¸€äº›éœ€è¦å±€éƒ¨è‡ªé€‚åº”æ³¨æ„åŠ›çš„ä»»åŠ¡ï¼ˆå¦‚ç‰©ä½“æ£€æµ‹ã€å¤šç›®æ ‡è·Ÿè¸ªï¼‰å¯èƒ½é™åˆ¶æ€§èƒ½ã€‚
	
- **AE å‹ç¼©åªé’ˆå¯¹ Q/Kï¼Œä¸è¦†ç›– V å’Œä¸­é—´ç»“æœ**
	- å¸¦å®½ç“¶é¢ˆå¯èƒ½åœ¨ä¸­é—´é˜¶æ®µè½¬ç§»åˆ° V æˆ– SÂ·V é˜¶æ®µï¼Œè€Œ AE ä¸»è¦å‹ç¼©äº† Q/Kã€‚
	
- **Reorder å¯¹å¤šå±‚ ViT çš„å…¨å±€ä¸€è‡´æ€§å½±å“æœªæ·±æŒ–**
	- ä¸åŒå±‚çš„æ³¨æ„åŠ›æ¨¡å¼å·®å¼‚å¾ˆå¤§ï¼Œæ–‡ä¸­ä¼¼ä¹æ˜¯å¯¹å•å±‚ mask è¿›è¡Œä¼˜åŒ–ï¼Œä½†å¯¹è·¨å±‚ mask å¤ç”¨çš„å½±å“åˆ†æä¸å¤Ÿæ·±å…¥ã€‚
	
---
(2) ç¡¬ä»¶å®ç°å±‚é¢
- **åŒå¼•æ“ï¼ˆDense/Sparse PEï¼‰èµ„æºåˆ©ç”¨ç‡é—®é¢˜**
	- åœ¨æŸäº›ç¨€ç–åº¦åˆ†å¸ƒä¸‹ï¼Œsparse engine çš„åˆ©ç”¨ç‡å¯èƒ½ä¸‹é™ï¼Œè€Œ dense engine å¯èƒ½é—²ç½®æˆ–é¥±å’Œï¼Œè¿™å¯¼è‡´ç¡¬ä»¶èµ„æºä¸å‡è¡¡ã€‚
		- é€‚åˆå›ºå®šæ¯”ä¾‹ dense/sparse çš„ä»»åŠ¡ï¼Œä½†è‹¥ç¨€ç–æ¯”ä¾‹æ³¢åŠ¨ï¼Œæ€§èƒ½å¯èƒ½ä¸ç¨³å®šã€‚
	
- **CSC ç¨€ç–æ ¼å¼å­˜å‚¨å¼€é”€**
	- CSC å¯¹ç¨€ç–åº¦é«˜çš„æƒ…å†µéå¸¸å¥½ï¼Œä½†å¦‚æœåç»­æ¨¡å‹ç¨€ç–åº¦ä¸‹é™ï¼Œç´¢å¼•å­˜å‚¨å¼€é”€ç›¸å¯¹å¢åŠ ã€‚
	
- **ç¼ºä¹å¤šä»»åŠ¡å¹¶è¡Œè°ƒåº¦æœºåˆ¶**
	- å½“å‰ pipeline é¢å‘å•è·¯ attentionï¼ŒVLM/å¤šæ¨¡æ€ä»»åŠ¡å¾€å¾€æœ‰å¤šè·¯ cross-attentionï¼Œéœ€è¦è°ƒåº¦å¤šä¸ª attention kernel å¹¶è¡Œå·¥ä½œã€‚
	
---
(3) ä»»åŠ¡/åº”ç”¨å±‚é¢
- **å¯¹ä¸‹æ¸¸ä»»åŠ¡æ³›åŒ–æœªéªŒè¯**
	- ä¸»è¦éªŒè¯æ˜¯åˆ†ç±»ä»»åŠ¡ï¼ˆImageNetï¼‰ï¼Œåœ¨æ£€æµ‹ã€åˆ†å‰²ã€è§†é¢‘ç†è§£ç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°æœªæ·±å…¥ç ”ç©¶ã€‚
		- ç¨€ç–æ¨¡å¼åœ¨éœ€è¦ä¿ç•™ç©ºé—´ç»“æ„ä¿¡æ¯çš„ä»»åŠ¡ï¼ˆæ¯”å¦‚ dense predictionï¼‰å¯èƒ½è¦é‡æ–°è®¾è®¡ã€‚
	
- **å¯¹ Token è¯­ä¹‰çš„æ•æ„Ÿæ€§ä¸è¶³**
	- Reorder è¿‡ç¨‹æ˜¯åŸºäº attention map çš„æ’åºï¼Œè€Œä¸æ˜¯ç›´æ¥è€ƒè™‘ token çš„è¯­ä¹‰ï¼ˆæ¯”å¦‚ç‰©ä½“è¾¹ç•Œã€æ–‡æœ¬åŒºåŸŸç­‰ï¼‰ã€‚
	
---
1. **æœªæ¥å¯èƒ½ Follow çš„æ–¹å‘**

æ–¹æ³•æ”¹è¿›
1. **åŠ¨æ€å¯è°ƒç¨€ç–æ¨¡å¼**
	- åœ¨æ¨ç†æ—¶æ ¹æ®è¾“å…¥å›¾ç‰‡çš„ä½æˆæœ¬ç‰¹å¾ï¼ˆå¦‚ä½åˆ†è¾¨ç‡ attention mapï¼‰è°ƒæ•´ sparse block çš„ maskã€‚
		- å¯å¼•å…¥è½»é‡çš„ Gumbel-Softmax / Top-K ç­›é€‰å™¨å®ç°åŠ¨æ€æ›´æ–°ã€‚
	
1. **è·¨å±‚ç¨€ç–æ¨¡å¼ååŒä¼˜åŒ–**
	- è€ƒè™‘å¤šå±‚ attention pattern çš„ç›¸å…³æ€§ï¼Œåœ¨è®­ç»ƒé˜¶æ®µä¼˜åŒ–ä¸€ä¸ªè·¨å±‚å…±äº«çš„å‹ç¼©/ç¨€ç–ç­–ç•¥ï¼Œå‡å°‘ mask å­˜å‚¨ã€‚
	
1. **å¤šåˆ†è¾¨ç‡/åˆ†å—é‡æ’**
	- åœ¨ reorder è¿‡ç¨‹ä¸­èåˆå¤šå°ºåº¦ token ä¿¡æ¯ï¼Œè®© global block æ›´å¥½åœ°è¦†ç›–å¤šå°ºå¯¸ç›®æ ‡ã€‚
	
1. **å…¨è·¯å¾„å‹ç¼©**
	- AE ä¸ä»…å‹ç¼© Q/Kï¼Œè¿˜å‹ç¼© V åŠä¸­é—´ S çŸ©é˜µï¼ˆå¯ä»¥ç”¨ä½ç§©åˆ†è§£ï¼‰ï¼Œè¿›ä¸€æ­¥é™ä½å¸¦å®½ã€‚
	
---
ç¡¬ä»¶ä¼˜åŒ–
1. **å¼¹æ€§åŒå¼•æ“è°ƒåº¦**
	- æ ¹æ®å®é™…ç¨€ç–æ¯”ä¾‹åŠ¨æ€è°ƒæ•´ dense/sparse engine çš„åˆ†é…æ¯”ä¾‹ï¼Œæå‡èµ„æºåˆ©ç”¨ç‡ã€‚
	
1. **å¤šè·¯ Attention å¹¶è¡ŒåŒ–**
	- é’ˆå¯¹ VLM ä¸­çš„ cross-attentionã€image-text attentionï¼Œè®¾è®¡å¤šè·¯å¹¶è¡Œçš„ sparse/dense è®¡ç®—é€šé“ã€‚
	
1. **æ–°ç¨€ç–å­˜å‚¨æ ¼å¼**
	- é’ˆå¯¹å›ºå®š maskï¼Œå¯é¢„ç¼–è¯‘æˆ PE-friendly çš„å‹ç¼©å¸ƒå±€ï¼Œå‡å°‘ç´¢å¼•è®¿é—®å»¶è¿Ÿã€‚
	
---
1. **åœ¨ VLM æ—¶ä»£çš„é€‚é…/æ”¹è¿›ç‚¹**

åœ¨ Vision-Language Models ä¸­ï¼ŒViTCoD è¿™ç±»ç»“æ„åŒ–ç¨€ç– + å‹ç¼©æŠ€æœ¯ä»ç„¶æœ‰ç”¨ï¼Œä½†è¦è§£å†³ä»¥ä¸‹é—®é¢˜ï¼š
1. **Cross-attention çš„ç¨€ç–æ¨¡å¼ä¸åŒ**
	- VLM ä¸­ image-to-text attention ä¸ image self-attention çš„ç¨€ç–åˆ†å¸ƒå·®å¼‚å¾ˆå¤§ã€‚
		- éœ€è¦å¯¹ä¸åŒç±»å‹çš„ attention åˆ†åˆ«è®¾è®¡ maskï¼Œæˆ–è€…åšå¤šæ¨¡æ€è”åˆ mask å­¦ä¹ ã€‚
	
1. **Token é•¿åº¦æ›´é•¿**
	- VLM å¾€å¾€è¾“å…¥é•¿æ–‡æœ¬ + é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œtoken æ•°é‡å¯è¾¾æ•°åƒç”šè‡³ä¸Šä¸‡ï¼Œç¨€ç–åŒ–å¸¦æ¥çš„æ”¶ç›Šä¼šæ›´æ˜æ˜¾ï¼Œä½† mask å­˜å‚¨/è°ƒåº¦æ›´å¤æ‚ã€‚
	
1. **å¤šæ¨¡æ€ token æ’åºé—®é¢˜**
	- ç°æœ‰ reorder é’ˆå¯¹è§†è§‰ tokenï¼Œå¦‚æœæ··åˆäº†æ–‡æœ¬ tokenï¼Œéœ€è¦ä¿æŒè·¨æ¨¡æ€ token å¯¹é½ï¼Œå¦åˆ™ cross-attention ä¿¡æ¯å¯èƒ½å—æŸã€‚
	
1. **å¯¹é½/è¯­ä¹‰ä¿æŒ**
	- VLM å¼ºè°ƒå›¾æ–‡å¯¹é½ï¼Œreorder å¦‚æœç ´åè§†è§‰ token ä¸æ–‡æœ¬ token çš„è¯­ä¹‰å¯¹åº”ï¼Œä¼šé™ä½æ¨¡å‹æ€§èƒ½ï¼Œéœ€è¦åŠ å…¥å¯¹é½çº¦æŸã€‚
	
---
## **ViT-slice: End-to-end Vision Transformer Accelerator with Bit-slice Algorithm**
---
## **FNM-Trans: Efficient FPGA-based Transformer Architecture with Full N:M Sparsity**
---
## **HG-PIPE: Vision Transformer Acceleration with Hybrid-Grained Pipeline**
---
## **FAS-Trans: Fully Exploiting FFN and Attention Sparsity for Transformer on FPGA**
---
## **FACT: FFN-Attention Co-optimized Transformer Architecture with Eager Correlation Prediction**
![image]({{ "images/TpsdbnDStoCzBSxZFhhcYIMFnsc.png" | relURL }})
---
### Background & Target
---
While the attention computation, focused by most previous works, only has decent power share when dealing with extremely long inputs. FACT, an efficient algorithm-hardware co-design optimizing all three modules of Transformer
---
### Algorithm
---
#### EP -- eager prediction
If there exists a few large probabilities in the ğ‘†, the rest are very small and can be safely skipped since they have little impact on the output.

Generating the QK matrices causes much more computation and power than the _ğ‘„ _Â· _ğ¾ ğ‘‡ _, leading to suboptimal improvement

EP with cross-stage log-based inner-product estimation, which can reduce not only attention score computation but also the _ğ‘„ğ¾ğ‘‰  _linear projection.

#### Attention-distribution-aware QKV Generation
åœ¨ $$
Eager\ Generation
$$
çš„æ—¶å€™ï¼Œå¯¹äºé¢„æµ‹æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ$$
\tilde{A}~
$$
,æ¯ä¸€è¡Œä¼šå– $$
top-k
$$
åšfilterï¼Œç„¶å $$
non-top-k
$$
å‘¢å°±ç›´æ¥skipæ‰ï¼Œé‚£ä¹ˆç”±äºè¿™æ ·å½¢æˆçš„ä¸€ä¸ªè·Ÿ $$
\tilde{A}
$$
ç»´åº¦ç›¸åŒçš„maskçŸ©é˜µè‡ªç„¶ä¼šå½¢æˆä¸€å®šçš„sparsityï¼Œæ¯”å¦‚ $$
\tilde{A}~
$$
çš„æŸä¸€åˆ—å…¨éƒ¨éƒ½è¢«skipæ‰çš„è¯ï¼Œé‚£ä¹ˆå¯¹åº”åæ¨ï¼Œåœ¨ $$
Q\ K\ V\ generation
$$
çš„æ—¶å€™ï¼Œåˆ©ç”¨ $$
\tilde{A}~
$$
äº§ç”Ÿçš„sparsity maskå°±å¯ä»¥å¿½ç•¥æ‰ä¸€éƒ¨åˆ†çš„ç²¾ç¡®è®¡ç®—ï¼Œä»è€Œå®ç° $$
Q K V\ generation / FFN\ layers
$$
æ›´å¥½çš„åŠ é€Ÿã€‚

$$
KV\ sparsity
$$
: It derives directly from the top-k result of the predicted _ğ´_Ë† matrix. If a column in the _ğ´_Ë† matrix has no values selected by the top-k, the key tensor related to this column is no longer required and can be safely pruned. Similarly, since the V matrix is multiplied by the attention matrix, the row with the same index in V matrix has no effect on the output, either, and can be safely removed.

$$
Q\ sparsity
$$
: Hence, when the _ğ´_Ë† matrix is obtained from EP, the difference between the 1 _ğ‘ ğ‘¡ _and 2 _ğ‘›ğ‘‘ _value of each row is compared to a threshold (we choose 3 based on experiments). If the former is larger, EP regards this row as being dominated by the largest token and directly uses a one-hot tensor as the _softmax _result where the largest token is assigned with 1.0 probability. In this way, the QK generation and attention computation related to this row can be fully skipped, and all that is needed is to copy the corresponding V tensor as the output.
#### Token-wise Mixed Precision FFN computation

---
### Hardware
---
#### EP unit with LOD circuit

#### KV-differential order (a scheduler to better match with the EP algorithm) 

#### Diagonal Storage Pattern for Mixed Precision FFN

---
### Limitations / Weakness / Further research 
---
The design concept of FACT and EP is via predicting the redundancy before computation, thus skipping unnecessary computation. FACTâ€™s EP is an output dynamic sparsity method. Further, EP prediction is a unique cross-stage method for Transformer

æ„Ÿè§‰token importanceçš„è¯„ä¼°æ–¹æ³•å¯ä»¥æœ‰æ‰€æ”¹è¿›ï¼Œå°¤å…¶æ˜¯FFNçš„è¿™ä¸ªåŠ é€Ÿæ–¹æ³•æ˜¯ä¸æ˜¯åº”è¯¥æœ‰æ›´åˆç†çš„åˆ¤æ–­æ–¹æ³•ï¼Ÿä¸çŸ¥é“æ˜¯å¦æ˜¯çœŸçš„æœ‰æ•ˆæœ

å¦å¤–QKVçš„sparseéƒ¨åˆ†æ„Ÿè§‰åšçš„å¾ˆä¸é”™ï¼Œå¯ä»¥æœ‰å‚è€ƒï¼Œä½†æ˜¯æ˜¯æŒ‰ç…§token-wiseè¿›è¡Œçš„ï¼Œç›´æ¥èˆå¼ƒæ‰å®Œæ•´tokenå¯¹åº”çš„Q/KVçš„generationï¼Œé‚£ä¹ˆå¦‚æœå¸¦å…¥åˆ°å¤šæ¨¡æ€é‡Œé¢çš„è¯ï¼Œç›´æ¥è¿›è¡Œè¿™æ ·æˆtokençš„pruneä¼šä¸ä¼šæ‰ç‚¹æ¯”è¾ƒä¸¥é‡ã€‚
## **Breaking the Low-Rank Dilemma of Linear Attention**
---
## **FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers**
---
## **Qwen2.5-VL Technical Report**
---
### window attention
#### computation complexity
1. self-attention

$$
\Omega(MSA) = 4HW\times d_h\times d_i + 2(HW)^2 \times d_i\\
$$

1. window-self-attention

$$
 \Omega(\text{1-MSA}) = 4M^2\times d_h\times d_i + 2({M^2})^2 \times d_i\\
$$


$$
\Omega(\text{W-MSA}) = 4HW\times d_h\times d_i + 2M^2HW\times d_i= HW \times(4d_hd_i+2M^2d_i)\\
$$


window attentionä¸ç­‰ä»·äºå¯¹MSAåštilingï¼ŒWMSAæ˜¯ç®—æ³•çº§åˆ«çš„æ³¨æ„åŠ›æ©ç ï¼ˆmaskï¼‰ï¼Œå®ƒç¡¬æ€§å±è”½äº†è·¨çª—å£çš„ token äº¤äº’ã€‚å³ä¾¿ä½ ç”¨ç›¸åŒçš„ MÃ—M å¤§å°åˆ‡ patchï¼Œå¦‚æœæ˜¯å…¨å±€ Attentionï¼Œå°±è¿˜æ˜¯èƒ½è·¨ patch äº’ç›¸æ³¨æ„ï¼›Window Attention åˆ™ä¸è¡Œã€‚

èƒ½å¦æœ‰çº¯ç¡¬ä»¶è°ƒåº¦æ¥æ”¯æŒattentionçš„è®¡ç®—å¤æ‚åº¦çš„é™ä½ï¼Ÿå¯èƒ½ç±»ä¼¼äºwindow MSAè¿™ç§ä¹ˆï¼ŒFlashattentionå°±æ˜¯çº¯HBMâ•ç®—æ³•è°ƒåº¦çš„ï¼Œä½†æ˜¯flashattentionåŸºæœ¬ä¸Šåšçƒ‚äº†
## BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models
---
## DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models
---
éå¸¸å–å·§çš„ä¸€ç¯‡æ–‡ç« ï¼Œæ²¡æœ‰æŒ‰ç…§å¸¸è§„çš„æ¯”å¦‚åˆ¤æ–­importanceçš„æ–¹æ³•ï¼Œè€Œæ˜¯æ¢äº†ä¸€ç§pruneçš„æ€è·¯ï¼Œé‡‡ç”¨æœ€å¤§åŒ–æœ€å°è·ç¦»ï¼Œæ¢å¥è¯è¯´å°±æ˜¯ç”¨è´ªå¿ƒç®—æ³•å–Visual tokençš„ç‰¹å®šå…ƒç´ æ•°é‡çš„subsetï¼Œä½¿å¾—æœ€ç»ˆè¯¥subsetå†…çš„å…ƒç´ ä¹‹é—´çš„è·ç¦»æœ€å¤§åŒ–ï¼ˆæœ€å¤šæ ·æ€§åŒ–ï¼‰
## ToDRE: Visual Token Pruning via Diversity and Task Awareness for Efficient Large Vision-Language Models
ä½œè€…ä¸æ˜¯å¾ˆdistinguished
### Background & Target
åŒæ ·æ˜¯è€ƒè™‘diversityçš„ä¸€ä¸ªpruneæ€è·¯ï¼Œ(citeäº†ä¸Šé¢çš„DivPruneï¼‰ï¼Œé¢å¤–è¿˜æœ‰token-task relevanceçš„ä¸€ä¸ªç»´åº¦ï¼Œæ ¹æ®ä¸¤ä¸ªç»´åº¦è¿›è¡Œprune
### Algorithm

## **FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression**
---





## **ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models**
---
## **Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to See**
---
## **Window Token Concatenation for Efficient Visual Large Language Models**
---
## **STAR: Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language Models Inference**
---
## **Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs**
---
## **Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture**
---



