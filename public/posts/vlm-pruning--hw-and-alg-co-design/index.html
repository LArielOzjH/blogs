<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/blogs/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=blogs/livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>VLM pruning : Hw and Alg co-design | LArielO&#39;s LAB</title>
<meta name="keywords" content="">
<meta name="description" content="VisionZip: Longer is Better but Not Necessary in Vision Language Models

Background
Algorithm
此外也是采用了merge，整篇文章没什么突出的算法核心点，但是工程、实验做的很扎实
[CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster

Background Analysis：**attention shift ****and **attention dispersion


**Attention shift: **** a tendency for textual attention to focus more on later parts of the visual token sequence, which is not desirable for preserving valuable visual information.**


**Attention dispersion:  **refers to the less concentrated attention distribution within the LLM compared to the visual encoder.">
<meta name="author" content="LArielO">
<link rel="canonical" href="http://localhost:1313/blogs/posts/vlm-pruning--hw-and-alg-co-design/">
<link crossorigin="anonymous" href="/blogs/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/blogs/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/blogs/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/blogs/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/blogs/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/blogs/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blogs/posts/vlm-pruning--hw-and-alg-co-design/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif:wght@400;600&family=Noto+Serif+SC:wght@400;600&family=IBM+Plex+Mono:wght@400;500;600&display=swap" rel="stylesheet">


<link rel="icon" type="image/x-icon" href="/blogs/favicon.ico">
<link rel="apple-touch-icon" sizes="180x180" href="/blogs/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/blogs/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/blogs/favicon-16x16.png">
<link rel="manifest" href="/blogs/site.webmanifest">


<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script','noscript','style','textarea','pre','code'],
      ignoreHtmlClass: 'tex2jax_ignore',
      processHtmlClass: 'tex2jax_process'
    },
    svg: { fontCache: 'global' }   
  };
</script>


<script defer src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>


</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/blogs/" accesskey="h" title="LArielO&#39;s LAB (Alt + H)">
                <img src="http://localhost:1313/blogs/android-chrome-512x512.png" alt="" aria-label="logo"
                    height="28">LArielO&#39;s LAB</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/blogs/" title="Main">
                    <span>Main</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/blogs/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blogs/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      VLM pruning : Hw and Alg co-design
    </h1>
    <div class="post-meta"><span title='2025-11-03 10:40:03 +0000 UTC'>November 3, 2025</span>&nbsp;·&nbsp;<span>12 min</span>&nbsp;·&nbsp;<span>2454 words</span>&nbsp;·&nbsp;<span>LArielO</span>

</div>
  </header> 
  <div class="post-content"><h2 id="visionzip-longer-is-better-but-not-necessary-in-vision-language-models"><strong>VisionZip: Longer is Better but Not Necessary in Vision Language Models</strong><a hidden class="anchor" aria-hidden="true" href="#visionzip-longer-is-better-but-not-necessary-in-vision-language-models">#</a></h2>
<hr>
<h3 id="background">Background<a hidden class="anchor" aria-hidden="true" href="#background">#</a></h3>
<h3 id="algorithm">Algorithm<a hidden class="anchor" aria-hidden="true" href="#algorithm">#</a></h3>
<p>此外也是采用了merge，整篇文章没什么突出的算法核心点，但是工程、实验做的很扎实</p>
<h2 id="heading"><strong>[CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster</strong><a hidden class="anchor" aria-hidden="true" href="#heading">#</a></h2>
<h2 id="image"><img alt="image" loading="lazy" src="/blogs/images/R2UWbLa34oFCItxsmIAcBwchnTc.png"><a hidden class="anchor" aria-hidden="true" href="#image">#</a></h2>
<h3 id="background-analysisand-attention-dispersion"><strong>Background Analysis：</strong><em>**attention shift **</em>**and **<em><strong>attention dispersion</strong></em><a hidden class="anchor" aria-hidden="true" href="#background-analysisand-attention-dispersion">#</a></h3>
<ol>
<li>
<p><em>**Attention shift: **</em>** a tendency for textual attention to focus more on later parts of the visual token sequence, which is not desirable for preserving valuable visual information.**</p>
</li>
<li>
<p><em>**Attention dispersion:  **</em><strong>refers to the less concentrated attention distribution within the LLM compared to the visual encoder.</strong></p>
</li>
</ol>
<h3 id="methods"><strong>Methods</strong><a hidden class="anchor" aria-hidden="true" href="#methods">#</a></h3>
<p><img alt="image" loading="lazy" src="/blogs/images/DgQMb6qIco9naKxhiWzc6TAHnWh.png">
核心的想法是用CLS attention 来决定prune掉的token（patch），经过 Visual Encoder 之后，取CLS attention，后R%的被prune掉（提到一个动态阈值的公式，没啥用），认为这些部分的patch对于整体的语义贡献很低。整体就是简单的思路采用CLS token的attention score去处理。</p>
<p>这里的问题就是到底是否能真正以CLS token的attention值来说明真正的importance然后做prune，这样做基本不需要硬件的额外支持</p>
<h3 id="scores"><strong>Scores</strong><a hidden class="anchor" aria-hidden="true" href="#scores">#</a></h3>
<p><img alt="image" loading="lazy" src="/blogs/images/VOZFbT9N3o9iaBxCQUicNL47nKe.png">
从ablation来看整个论文的思路基本都是在考虑：</p>
<ol>
<li>
<p>prune的位置：LLM浅层 OR LLM之前（visual encoder之后）</p>
</li>
<li>
<p>判断prune token的方式：[CLS] token attention OR random OR patch attention</p>
</li>
</ol>
<p>没有考虑的地方：</p>
<ol>
<li>
<p>head attention ：加入head的充分考虑，比如HeatViT那种加入head score的or so</p>
</li>
<li>
<p>prune的只能是token-wise的么？有论文会在d维度做适当的prune，或者head维度做prune，也都是一些常见的思路，另外结构化稀疏性肯定是要优先考虑的，比如可以细粒度结构化之类的，参考sanger非常经典，但是做的也比较多</p>
</li>
<li>
<p>大多论文都采用prune+merge的模式，能不能改变这个模式，能把prune掉的token information利用起来（如果是token-wise的话）或者充分利用prune掉的其他信息，比如用可学习的小型网络等来学习一些模式，把prune掉的用比较efficient的方式恢复出来。</p>
</li>
<li>
<p>有没有可能摆脱importance做prune的选择，比如#20DiVprune是一个很取巧的方式</p>
</li>
<li>
<p>text-agnostic肯定是一种更为accuracy-friendly的算法，那么能不能有一种方法，先prune掉一部分token（if token-wise)然后再想办法根据text token去恢复或者更好的提取non-informative token或者进一步做更合适的prune，是一个2-stage的方法</p>
</li>
<li>
<p>可以调查一下对于VLM计算复杂度最大的地方，二次方的attention肯定是要解决的，可是如果能有办法优化后续的大规模的FFN那肯定是更efficient的</p>
</li>
</ol>
<h2 id="sparsevlm-visual-token-sparsification-for--efficient-vision-language-model-inference_"><strong>SPARSEVLM: VISUAL TOKEN SPARSIFICATION FOR  EFFICIENT VISION-LANGUAGE MODEL INFERENCE_VVVI</strong><a hidden class="anchor" aria-hidden="true" href="#sparsevlm-visual-token-sparsification-for--efficient-vision-language-model-inference_">#</a></h2>
<hr>
<h3 id="background--target">Background &amp; Target<a hidden class="anchor" aria-hidden="true" href="#background--target">#</a></h3>
<p>we propose an efficient training-free token optimization mechanism dubbed SparseVLM without extra parameters or fine-tuning costs. Concretely, given that visual tokens complement text tokens in VLMs for linguistic reasoning, we select visual-relevant text tokens to rate the significance of vision tokens within the self-attention matrix extracted from the VLMs. Then we progressively prune irrelevant tokens.</p>
<p>SparseVLM 是一个训练免（training-free）、文本引导（text-aware）的视觉 token 稀疏化框架：
先挑出“和图像强相关”的文本 token 当评审（raters），用它们在解码器里的跨模态注意力给每个视觉 token 打分；然后逐层按打分和一个秩（rank）自适应规则删掉不重要的视觉 token；被删的 token 不直接丢，而是回收—聚类—重构成少量“压缩 token”再放回，尽量不丢信息。这样能显著减 FLOPs/显存/时延，同时保持高精度。</p>
<h3 id="algorithm-1">Algorithm<a hidden class="anchor" aria-hidden="true" href="#algorithm-1">#</a></h3>
<p>非常好的一个text-agnostic算法，大概率我最后会follow这个做一些工作</p>
<h4 id="raters">Raters<a hidden class="anchor" aria-hidden="true" href="#raters">#</a></h4>
<p>text-aware的核心是根据text tokens有针对性的对visual tokens做pruning，本文的一个重要算法是首先在进入LLM之前进行一次raters的选择，raters是一部分text tokens，叫做“评委”，通常来说一句text中真正对图像token选取有指导意义的并没有几个，比如大量的冠词，介词等都是不重要的可以忽略的，因此要选择raters，先从整句文本里评估每个词和视觉的关联度$$
R
$$
，只保留高于平均值 $$
m=Mean(R)
$$
的候选词当评委，减少无关词对打分的噪声；评委集合用策略 $$
S
$$
确定，整步只在进入解码器之前做一次，开销 $$
O(Lt\ · Lv\  · D)
$$</p>
<h4 id="rate-the-visual-tokens-with-scores">Rate the visual tokens with scores<a hidden class="anchor" aria-hidden="true" href="#rate-the-visual-tokens-with-scores">#</a></h4>
<p>对每个视觉 token j，把评委（文本）对它的注意力分数按行求平均，得到第 j 个视觉 token 的综合价值分，然后根据这个得分进行prune，具体prune掉多少个呢，用到了一个rank-based的方法，比如如果P矩阵是接近满秩的话，那么就说明都是线性无关的，就基本不需要prune，保留基本所有visual tokens。如果P是低秩的话，那么就多prune掉一些，根据注意力分数。 $$
N=α⋅(Lv−Rank(P))
$$</p>
<h4 id="merge">Merge<a hidden class="anchor" aria-hidden="true" href="#merge">#</a></h4>
<p>也是按照分数merge，做一次k 近邻密度峰聚类：</p>
<h2 id="madtp-multimodal-alignment-guided-dynamic-token-pruning-for--accelerating-vision-language-transformer"><strong>MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for  Accelerating Vision-Language Transformer</strong><a hidden class="anchor" aria-hidden="true" href="#madtp-multimodal-alignment-guided-dynamic-token-pruning-for--accelerating-vision-language-transformer">#</a></h2>
<hr>
<p><img alt="image" loading="lazy" src="/blogs/images/YRIEbmTgqorENvx2tmDcVrIbnff.png"></p>
<h3 id="mag"><strong>MAG</strong><a hidden class="anchor" aria-hidden="true" href="#mag">#</a></h3>
<h3 id="dtp"><strong>DTP</strong><a hidden class="anchor" aria-hidden="true" href="#dtp">#</a></h3>
<p><strong>事实证明，单模态压缩中的动态令牌修剪比静态令牌修剪更有效，因为它可以根据输入实例的复杂程度自适应调整模型的压缩率。</strong>
<img alt="image" loading="lazy" src="/blogs/images/GZqrbdyPSoDvgAx8octcFCM0nvE.png"></p>
<h3 id="tis"><strong>TIS</strong><a hidden class="anchor" aria-hidden="true" href="#tis">#</a></h3>
<p>$$
TIS = (Scls + Sself + Stoken)/3</p>
<p>$$</p>
<p><strong>比如拿 visual 模态来举例：</strong></p>
<h2 id="vscan-rethinking-visual-token-reduction-for-efficient-large-vision-language-models"><strong>VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models</strong><a hidden class="anchor" aria-hidden="true" href="#vscan-rethinking-visual-token-reduction-for-efficient-large-vision-language-models">#</a></h2>
<h2 id="image-1"><img alt="image" loading="lazy" src="/blogs/images/PzhNbrL9SocCajxN5CncqrLDnJe.png"><a hidden class="anchor" aria-hidden="true" href="#image-1">#</a></h2>
<h3 id="observations"><strong>Observations</strong><a hidden class="anchor" aria-hidden="true" href="#observations">#</a></h3>
<ol>
<li>
<p><strong>In the visual encoding stage, the visual encoder attends to locally significant tokens in the shallow layers, focusing on fine-grained local details, while at deeper layers, it gradually shift its focus to a highly condensed set of tokens that encapsulate broader global context;（visual encoder里面随着层数增多，注意力呈现的变化趋势,下面两个图我自己可视化了一下，可以看到attention focus由广泛到集中，但问题是集中的部分并不完全是我们人眼所普遍认为的focus）</strong></p>
</li>
<li>
<p><strong>In the LLM decoding stage, early layers exhibit strong positional bias toward visual tokens appearing later in the sequence, neglecting their semantic relevance; as the layers deepen, cross-modal interactions begin to emerge, and output token probabilities typically converge in the mid-to-late layers where visual information is more effectively integrated into the language stream.（和FsterVLM里面提到的attention dispersion是一个东西）</strong></p>
</li>
</ol>
<h3 id="current-methods-limitations"><strong>Current methods limitations</strong><a hidden class="anchor" aria-hidden="true" href="#current-methods-limitations">#</a></h3>
<p><strong>可以看到这里用了这样一张图片和query来disable之前的几种主流方法</strong></p>
<p><img alt="image" loading="lazy" src="/blogs/images/ZtWWbX6aHo8m1Cx9xQdcZulHnEe.png"></p>
<p><strong>然后做了3个study：各种可视化去探究LLM对于textual和visual信息的处理随着层数变化的改变情况</strong>
<img alt="image" loading="lazy" src="/blogs/images/OQvkb3nnuopOgrxweM2cPr0sn5I.png">
<strong>左边的图其实就是attention shift的可视化（位置编码的影响），随着LLM layers增多，这种现象逐渐diminish</strong></p>
<p><strong>右边的图：We observe that the middle LLM layers are primarily responsible for interacting with the visual tokens, whereas the early and deep layers focus predominantly on processing textual information.就是LLM的中间的那些layers会更倾向于结合visual信息聚合处理，然而shallow/deep layer都会focus更多在textual信息上</strong>
<img alt="image" loading="lazy" src="/blogs/images/IO9vbikGqoUEXwxNisucXMZHnG0.png">
<strong>We observe that in more challenging open-ended tasks like GQA, the next-token predictions stabilize around LLM layer 20, whereas in simpler yes/no tasks such as POPE, the predictions converge earlier, around LLM layer 16.</strong></p>
<p><strong>在这一部分得出的结论就是：LLM的early layers并不是最适合pruning的层数位置，因为</strong></p>
<ol>
<li>
<p>** positional bias；**</p>
</li>
<li>
<p>** limited engagement of visual content**</p>
</li>
</ol>
<p><strong>而middle layers才是最好的，因为：</strong></p>
<ol>
<li>
<p><strong>better preserves critical cross-modal interactions</strong></p>
</li>
<li>
<p><strong>minimizes disruption to model predictions（太深层起不到太好的pruning效果而且可能会有disrupt）</strong></p>
</li>
</ol>
<h3 id="methods-1"><strong>Methods</strong><a hidden class="anchor" aria-hidden="true" href="#methods-1">#</a></h3>
<p><img alt="image" loading="lazy" src="/blogs/images/MtSRbnAfHoWKSexJlXGctCgDn4e.png"></p>
<h4 id="reducing-visual-redundancy-via-complementary-global-and-local-scans"><strong>Reducing Visual Redundancy via Complementary Global and Local Scans</strong><a hidden class="anchor" aria-hidden="true" href="#reducing-visual-redundancy-via-complementary-global-and-local-scans">#</a></h4>
<h5 id="global-scan"><strong>Global Scan</strong><a hidden class="anchor" aria-hidden="true" href="#global-scan">#</a></h5>
<p><strong>因为visual encoder的最后一层（或者倒数第二层）是global content，所以采用跟之前工作类似的方法CLS attention的方式来选择 global tokens （g）</strong></p>
<h5 id="local-scan"><strong>Local Scan</strong><a hidden class="anchor" aria-hidden="true" href="#local-scan">#</a></h5>
<p><strong>因为前面已经分析过了直接使用CLS attention获取全局的信息会使得一些重要的细节信息遗漏掉，所以还要在visual encoder的浅层来获取一些finer的细节信息，具体选择方式是按照windows去做选择（l），g和l的数量被控制为一样</strong></p>
<h5 id="token-merging"><strong>Token Merging</strong><a hidden class="anchor" aria-hidden="true" href="#token-merging">#</a></h5>
<p><strong>把未被选择的那些token与被选择的token做内积，对每个unselected token选择最相似的selected token并且做average merge得到最终的merged representation（感觉这个地方在arch层面可以考虑适当的优化）</strong></p>
<h4 id="reducing-textual-irrelevance-via-middle-layer-pruning"><strong>Reducing Textual Irrelevance via Middle Layer Pruning</strong><a hidden class="anchor" aria-hidden="true" href="#reducing-textual-irrelevance-via-middle-layer-pruning">#</a></h4>
<p><strong>Further refine the token set based on their relevance to the text query</strong></p>
<h3 id="experiment-scores"><strong>Experiment Scores</strong><a hidden class="anchor" aria-hidden="true" href="#experiment-scores">#</a></h3>
<h2 id="skip-vision-efficient-and-scalable-acceleration-of-vision-language-models-via-adaptive-token-skipping"><strong>Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping</strong><a hidden class="anchor" aria-hidden="true" href="#skip-vision-efficient-and-scalable-acceleration-of-vision-language-models-via-adaptive-token-skipping">#</a></h2>
<h2 id="image-2"><img alt="image" loading="lazy" src="/blogs/images/IjGpbXnSroa5uoxmbiAcdZzIn9g.png"><a hidden class="anchor" aria-hidden="true" href="#image-2">#</a></h2>
<h2 id="accelerating-pre-training-of-multimodal-llms-via-chain-of-sight"><strong>Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight</strong><a hidden class="anchor" aria-hidden="true" href="#accelerating-pre-training-of-multimodal-llms-via-chain-of-sight">#</a></h2>
<h2 id="image-3"><img alt="image" loading="lazy" src="/blogs/images/DI3hbanZloB0KCxy4iccxpavnEe.png"><a hidden class="anchor" aria-hidden="true" href="#image-3">#</a></h2>
<h3 id="brief-introduction-">**Brief Introduction **<a hidden class="anchor" aria-hidden="true" href="#brief-introduction-">#</a></h3>
<p><strong>用更少的visual tokens训练通常意味着perfomance的下降，那么有没有一种方式能够解决这个问题，用更少的visual tokens去包含更多的信息，且能不受input resolution的影响，从而实现更efficient的pre-training？</strong></p>
<p><strong>CoS(chain of sight)就是这样一个方式，这是一个vision-language bridge的模块，整体的思路有点类似之前的Perceiver抑或是Q-former，但还有一个特点是对预训练和微调部分使用的token有很大差别，后者使用更多更fine的tokens，以获取更finer的vision信息，从而弥补perfomance可能的掉点</strong>
<img alt="image" loading="lazy" src="/blogs/images/HSSVbssrXodcvmxT2z4ciYR5n8d.png"></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>The core mechanism is our multi-scale visual resampler, which produces visual tokens 
</span></span><span style="display:flex;"><span>of multiple visual scales. Inspired by the classical concept of multi-scale feature hierarchy in visual 
</span></span><span style="display:flex;"><span>understanding [105, 41, 32, 106, 82, 52], we partition the visual features produced by the visual 
</span></span><span style="display:flex;"><span>backbone using windows of multiple sizes. For each window size, a visual resampler is implemented 
</span></span><span style="display:flex;"><span>to produce a specified number of visual tokens per window. Subsequently, the visual tokens from 
</span></span><span style="display:flex;"><span>various window sizes are gathered and linked in a global-to-local manner, forming a chain of reasoning 
</span></span><span style="display:flex;"><span>steps from coarse views gradually to fine-grained perspectives.
</span></span><span style="display:flex;"><span>On top of this, we propose a post-pretrain token scaling strategy, which compounds the elements of 
</span></span><span style="display:flex;"><span>input resolution and window size manipulation to enable a significant escalation in the token count 
</span></span><span style="display:flex;"><span>for our Chain-of-Sight, reaching up to 16× increase during fine-tuning. Such adaptability allows for 
</span></span><span style="display:flex;"><span>the fine-tuning of the model with a flexible granularity or complexity as required, without the the 
</span></span><span style="display:flex;"><span>necessity for an additional pre-training phase.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>图像 → ViT视觉编码器 → Multi-Scale Visual Resampler → 少量 token 输入语言模型 → 预训练  
</span></span><span style="display:flex;"><span>                                               ↓  
</span></span><span style="display:flex;"><span>                      （微调时再放大 token 数量） → 多 token 输入语言模型 → 下游任务微调
</span></span></code></pre></div><h3 id="methods-delineate"><strong>Methods Delineate</strong><a hidden class="anchor" aria-hidden="true" href="#methods-delineate">#</a></h3>
<h4 id="multi-scale-visual-resamplers"><strong>Multi-scale visual Resamplers</strong><a hidden class="anchor" aria-hidden="true" href="#multi-scale-visual-resamplers">#</a></h4>
<h4 id="post-pretrain-token-scaling"><strong>Post-Pretrain Token Scaling</strong><a hidden class="anchor" aria-hidden="true" href="#post-pretrain-token-scaling">#</a></h4>
<p><img alt="image" loading="lazy" src="/blogs/images/BPBibeDZZokNk6xe0LycsqwXnuc.png"></p>
<ol>
<li>
<p><strong>在预训练中只用少量视觉 token（如 32/80），大幅加速训练；</strong></p>
</li>
<li>
<p><strong>微调阶段再将 token 数扩大（通过调整输入分辨率 + 减小窗口 size）；</strong></p>
</li>
<li>
<p><strong>提出 compound scaling：resolution scaling × window scaling，最多可将 token 数扩大 16 倍（如 32 → 512）；</strong></p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>Step 1: 预训练阶段
</span></span><span style="display:flex;"><span>    图像分辨率低（224×224）
</span></span><span style="display:flex;"><span>    window size 粗（16、8）
</span></span><span style="display:flex;"><span>    每个 window 分配少量 query
</span></span><span style="display:flex;"><span>    → 得到少量（如 32、80）视觉 token
</span></span><span style="display:flex;"><span>    → 快速预训练
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Step 2: 微调阶段
</span></span><span style="display:flex;"><span>    提高分辨率（如 448×448）
</span></span><span style="display:flex;"><span>    引入更小的 window size（8、4、2）
</span></span><span style="display:flex;"><span>    每个 window 分配更多 query
</span></span><span style="display:flex;"><span>    → 得到更多（如 336、528、1296）视觉 token
</span></span><span style="display:flex;"><span>    → 细粒度理解图像，提高任务性能
</span></span></code></pre></div><h2 id="heatvit-hardware-efficient-adaptive-token-pruning-for-vision-transformers"><strong>HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers</strong><a hidden class="anchor" aria-hidden="true" href="#heatvit-hardware-efficient-adaptive-token-pruning-for-vision-transformers">#</a></h2>
<hr>
<h3 id="background--target-1">Background &amp; Target<a hidden class="anchor" aria-hidden="true" href="#background--target-1">#</a></h3>
<p>_<strong>HeatViT</strong>__: _While vision transformers (ViTs) have continuously achieved new milestones in the field of computer vision, their sophisticated network architectures with high computation and memory costs have impeded their deployment on resource-limited edge devices</p>
<p>we propose a hardware-efficient image-adaptive token pruning framework called HeatViT for efficient yet accurate ViT acceleration on embedded FPGAs.</p>
<p>_**SPViT: **_high computation and memory cost</p>
<p>a dynamic attention-based multi-head token selector, which is a lightweight module for adaptive instance-wise token selection. We further introduce a soft pruning technique, which integrates the less informative tokens chosen by the selector module into a package token rather than discarding them completely</p>
<p>project: <a href="https://github.com/PeiyanFlying/SPViT">https://github.com/PeiyanFlying/SPViT</a></p>
<h3 id="algorithm-2">Algorithm<a hidden class="anchor" aria-hidden="true" href="#algorithm-2">#</a></h3>
<h4 id="_head-evaluation-multi-head-_token-classifier">_Head-Evaluation Multi-Head _<em>Token Classifier</em><a hidden class="anchor" aria-hidden="true" href="#_head-evaluation-multi-head-_token-classifier">#</a></h4>
<p><em><strong>SPViT:</strong></em></p>
<p>整体呢就是采用一个可学习的网络（主要就是几层很小的MLP，实际计算量不足ViT的1%），用这个网络去学习keep/prune的规则、模式，算法思路上就是每个head关注的特征和部分是不一样的，也就是每个head本身对于所有token是自带一定注意力的，可以学习到head score那么自然就会想到在最后concat的时候采用加权平均的方式，也就是用 $$
head\ score \times token \ score \ for each \ head
$$
来表示
<img alt="image" loading="lazy" src="/blogs/images/UeyTbGK3WooyitxO2oMcFFf1nCd.png"></p>
<ol>
<li>
<p>$$
MLP_1:LayerNorm \rightarrow Linear(d,d/2) \rightarrow GELU
$$</p>
</li>
<li>
<p>$$
MLP_2:Linear(d,d/2) \rightarrow GELU \rightarrow Linear(d/2,d/4) \rightarrow GELU \rightarrow Linear(d/4,2)
$$</p>
</li>
<li>
<p>$$
MLP_3:Linear(H,H/2) \rightarrow GELU \rightarrow Linear(H/2,H) \rightarrow Sigmoid
$$</p>
</li>
<li>
<p>$$
f_i^{local}=MLP_1(x_i) \in \mathbb{R}^{N \times d/2}
$$</p>
</li>
<li>
<p>$$
f_i^global=AvgPool(MLP_1(x_i),D) \in \mathbb{R}^{1 \times d/2}
$$</p>
</li>
<li>
<p>$$
f_i=[f_i^local,f_i^global] \in \mathbb{R}^{N \times d}
$$</p>
</li>
<li>
<p>$$
t_i=Softmax(MLP_2(f_i)) \in \mathbb{R}^{N \times 2}
$$</p>
</li>
<li>
<p>$$
\bar{X}=AvgPool(X) \in \mathbb{R}^{N \times H}
$$</p>
</li>
<li>
<p>$$
A=MLP_3(\bar{X})
$$</p>
</li>
<li>
<p>$$
\tilde{T} = \frac{\sum_{i=1}^{H} t_i \ast a_i}{\sum_{i=1}^{H} a_i} \in \mathbb{R}^{N \times 2}
$$</p>
</li>
<li>
<p>$$
D=GumbelSoftmax( \tilde{T}) \in {0,1}^N
$$</p>
</li>
</ol>
<p>1-3是MLP的declaration，4-7是token score branch，8-10是head score branch</p>
<p>MLP都用降维，因为不是要提取更详细更复杂的特征信息，目标是生成一个简单、清晰、低维的策略 score，而不是保留原始语义。head score最后activation用了Sigmoid，为了便于加权</p>
<p>token score的部分，为什么保留到2维？这两维分别用于表示keep/prune的概率，这也是这个网络最核心期望学习到的特征，另一方面后续会用到Gumbel Softmax，为了反向传播学习，而不是直接用argmax等来决定是prune还是keep，Gumbel Softmax 是解决“可微分离散选择”的标准做法，SPViT 通过Gumbel Softmax实现了 token pruning 的 end-to-end 训练和部署闭环。</p>
<p><em><strong>HeatViT:</strong></em></p>
<h4 id="token-packaging-technique">Token Packaging Technique<a hidden class="anchor" aria-hidden="true" href="#token-packaging-technique">#</a></h4>
<p>做这部分packager的原因很简单：1. 直接prune的话仍然是会丢掉不少信息，所以想聚合一下，不直接丢掉信息；2. 另外在transformer里，earlier block受到这种info的丢失影响会更显著；3. 此外在“Instance localization for self-supervised detection pre-training”这篇文章提到了，background信息的过多剔除会导致self-attention提取关键特征的能力降低；4. 结合这些被prune掉的tokens的信息到一个统一的package token里，保有一定的语义信息</p>
<h3 id="hardware">Hardware<a hidden class="anchor" aria-hidden="true" href="#hardware">#</a></h3>
<p>硬件主要做了几部分优化：1. 控制流的设计去尽可能多的复用ViT已有的backbone部件；2. 并行化的优化，对于GEMM支持多头的并行处理；3. 优化非线性运算操作；4. LayerNorm是在CPU上做的
<img alt="image" loading="lazy" src="/blogs/images/SY4EbGvNooVYiHxdTXNcVt8LnVd.png"></p>
<h3 id="limitations--weakness--further-research">Limitations / Weakness / Further research<a hidden class="anchor" aria-hidden="true" href="#limitations--weakness--further-research">#</a></h3>
<p>HeatViT是用可学习的小型网络来选择prune/keep的token，但缺点就是对于特定的图片，在inference的时候整个网络的focus是固定的，但在VLM中实际的focus必然离不开prompt的语言token部分</p>
<h2 id="vitality-unifying-low-rank-and-sparse-approximation-for-vision-transformer-acceleration-with-a-linear-taylor-attention"><strong>ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention</strong><a hidden class="anchor" aria-hidden="true" href="#vitality-unifying-low-rank-and-sparse-approximation-for-vision-transformer-acceleration-with-a-linear-taylor-attention">#</a></h2>
<hr>
<h3 id="background--target-2">Background &amp; Target<a hidden class="anchor" aria-hidden="true" href="#background--target-2">#</a></h3>
<p>we propose a first-of-its-kind algorithm-hardware co-designed framework, dubbed VITALITY, for boosting the inference efficiency of ViTs. Unlike sparsity-based Transformer accelerators for NLP, VITALITY unifies both low-rank and sparse components of the attention in ViTs.</p>
<h3 id="algorithm-3">Algorithm<a hidden class="anchor" aria-hidden="true" href="#algorithm-3">#</a></h3>
<h4 id="mean-centered-k">Mean-centered K<a hidden class="anchor" aria-hidden="true" href="#mean-centered-k">#</a></h4>
<h4 id="taylor-softmax-attention">Taylor-softmax attention<a hidden class="anchor" aria-hidden="true" href="#taylor-softmax-attention">#</a></h4>
<p>ViTality想用线性注意力，选择采用taylor来近似exp()，理由很简单，进行了mean-center后的注意力分数大多集中在[-1,1]之间，而在0附近采用Taylor来做近似exp()是不错的选择，所以核心的思路就是用taylor展开的一阶项来近似注意力分数处于[-1,1]之间的weak connection部分，那自然会想到用高阶项来近似strong connection部分，但问题又出现了，高阶的计算及其复杂，很可能就offset掉linear attention带来的优势了，所以需要找方法替代高阶/strong connection的计算，ViTality选择的方法是用Sanger（调整threshold）来以Sparse attention的方式近似处理strong connection。所以一句话总结ViTality算法层面就是：用unified的Linear Taylor Attention以及Sparse Attention来替代传统的Softmax Attention来减小计算复杂度 $$
O(n^2) \rightarrow O(n)
$$
。
<img alt="image" loading="lazy" src="/blogs/images/FxezbG1ZAoCDCwxGO0DcsceKn3c.png">
另外注意：ViTality在训练阶段采用的是Linear+Sparse的形式做训练，后者能起到正则化的作用。但是在推理的时候仅仅考虑Linear的部分，而忽视掉了Sparse部分，定然会有些许的掉点但是考虑到其他因素可以接受。</p>
<h3 id="hardware-1">Hardware<a hidden class="anchor" aria-hidden="true" href="#hardware-1">#</a></h3>
<p>micro-architecture
<img alt="image" loading="lazy" src="/blogs/images/N6ZsbmjUFoLOPLxe5BgcY1s3nqg.png"></p>
<h4 id="多块式设计chunk-based-design">多块式设计（Chunk-based Design）<a hidden class="anchor" aria-hidden="true" href="#多块式设计chunk-based-design">#</a></h4>
<ul>
<li>不用一个可重构处理阵列去跑所有操作（这样开销大），而是分成多块：
<ul>
<li>大阵列（SA-General）：负责大规模矩阵乘法，比如 <code>QG</code>、<code>K̂^T V</code>。
<ul>
<li>小阵列（SA-Diag）：负责对角矩阵乘法（如 <code>Q k̂_sum^T</code>），乘法量小得多，只用一列PE。</li>
<li>累加器阵列（Accumulator Array）：列方向求和，用于 <code>k̂_sum</code> 和 <code>v_sum</code> 等。</li>
<li>除法阵列（Divider Array）：两种模式——单除数（均值化） &amp; 多除数（Taylor 分子分母相除）。</li>
<li>加法阵列（Adder Array）：元素级加/减（如均值中心化、Taylor 分子分母加法）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>优势：</p>
<ul>
<li>
<p>专用单元避免大阵列跑小任务的浪费。</p>
</li>
<li>
<p>小阵列、轻运算单元功耗低、面积占用小。</p>
</li>
<li>
<p>不同块可并行执行，方便流水线。</p>
</li>
</ul>
<hr>
<h4 id="四级存储层次memory-hierarchy">四级存储层次（Memory Hierarchy）<a hidden class="anchor" aria-hidden="true" href="#四级存储层次memory-hierarchy">#</a></h4>
<ul>
<li>
<p>DRAM → SRAM → NoC → 寄存器（Regs）</p>
<ul>
<li>DRAM：大容量存储
<ul>
<li>SRAM：片上缓存，减少DRAM访问</li>
<li>NoC：片内传输</li>
<li>Regs：每个计算单元局部寄存器，配合 systolic array 数据复用</li>
</ul>
</li>
</ul>
</li>
<li>
<p>数据复用优化：</p>
<ul>
<li>矩阵乘法时，让权重或中间结果在PE内驻留（stationary）以减少访存。
<ul>
<li><code>V</code> 在计算 <code>K̂^T V</code> 时驻留，<code>G</code> 在计算 <code>QG</code> 时驻留。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="流水线创新intra-layer-pipeline">流水线创新（Intra-Layer Pipeline）<a hidden class="anchor" aria-hidden="true" href="#流水线创新intra-layer-pipeline">#</a></h4>
<hr>
<h4 id="数据流创新down-forward-accumulation-dataflow">数据流创新（Down-Forward Accumulation Dataflow）<a hidden class="anchor" aria-hidden="true" href="#数据流创新down-forward-accumulation-dataflow">#</a></h4>
<p><img alt="image" loading="lazy" src="/blogs/images/JSx0b7dyYoGy5Mx5eBycHMj2nPg.png"></p>
<ul>
<li>常见两种数据流：
Output Stationary：输出留在PE内（内累加）	Input Stationary：输入权重留在PE内（行/列移动，向下累加）</li>
<li>ViTALiTy 的选择：
<ul>
<li>全部矩阵乘法统一用 Input Stationary + Down-forward accumulation。
<ul>
<li>好处：</li>
<li>不用为不同矩阵乘法切换累加模式 → 简化PE设计。
<ul>
<li>降低 systolic array 功耗（实验表明 systolic array 的能耗占总能耗大头）。</li>
<li>代价：</li>
</ul>
</li>
<li><code>G</code> 不驻留 → 访存量增加，但总能耗反而下降（因为矩阵乘法能耗减少更多）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="limitations--weakness--further-research-1">Limitations / Weakness / Further research<a hidden class="anchor" aria-hidden="true" href="#limitations--weakness--further-research-1">#</a></h3>
<p>在处理弱连接的时候认为[-1,1]是weak，并用一阶来近似，有一个问题是注意力分布不见得都是大部分为弱连接的，可能随着输入patch或者是不同head等等的变化，甚至会出现大部分为强连接，那这时的掉点可以预想会很严重。&mdash;&mdash;static 判定的不足。</p>
<p>稀疏部分在训练微调有用，但是在推理的时候用不到：说明sparse attention很可能只起到了正则化作用，而缺乏提供足够语义信息的能力&mdash;可能在一些强局部相关的任务会表现比较差</p>
<h2 id="vitcod-vision-transformer-acceleration-via-dedicated-algorithm-and-accelerator-co-design"><strong>ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design</strong><a hidden class="anchor" aria-hidden="true" href="#vitcod-vision-transformer-acceleration-via-dedicated-algorithm-and-accelerator-co-design">#</a></h2>
<hr>
<h3 id="background--target-3">Background &amp; Target<a hidden class="anchor" aria-hidden="true" href="#background--target-3">#</a></h3>
<p>ViTs have a relatively fixed number of input tokens, whose attention maps can be pruned by up to 90% even with fixed sparse patterns, without severely hurting the model accuracy</p>
<h3 id="algorithm-4">Algorithm<a hidden class="anchor" aria-hidden="true" href="#algorithm-4">#</a></h3>
<p>ViTCoD的核心算法是split and conquer &amp; auto encoder</p>
<h4 id="vitcods-split-and-conquer-algorithm--hardware"><em>ViTCoD’s Split and Conquer Algorithm / hardware</em><a hidden class="anchor" aria-hidden="true" href="#vitcods-split-and-conquer-algorithm--hardware">#</a></h4>
<p>ViTCoD采用fixed mask的稀疏格式，扔进去所有数据，算出每一个注意力分数矩阵$$
A
$$
并且求平均，根据这个平均值注意力分数矩阵$$
\bar{A}
$$
来构建一个fixed mask，并且在推理的时候直接用这个mask固定化稀疏模式</p>
<h4 id="vitcod-learnable-auto-encoder-module"><em>ViTCoD Learnable Auto-encoder Module</em><a hidden class="anchor" aria-hidden="true" href="#vitcod-learnable-auto-encoder-module">#</a></h4>
<h3 id="hardware-2">Hardware<a hidden class="anchor" aria-hidden="true" href="#hardware-2">#</a></h3>
<h4 id="why">Why<a hidden class="anchor" aria-hidden="true" href="#why">#</a></h4>
<ul>
<li>ViTCoD 的 S&amp;C 让注意力变成固定且结构化的稀疏，再配合 AE 压缩 Q/K，给硬件提供了两类新机会：
① 固定稀疏图不再需要在线预测、控制更简单；② Q/K 可被压缩减少数据搬运。然而视觉里的稀疏注意力常沿对角线集中，会造成数据重用差、PE 利用率低且带宽受限，因此需要“算法+加速器”协同设计（文中 roofline 分析明确：仅稀疏还会更受带宽限制，必须再降通信）。</li>
</ul>
<h4 id="dataflows-stationary-vs-k-stationary">Dataflow：S-stationary vs K-stationary<a hidden class="anchor" aria-hidden="true" href="#dataflows-stationary-vs-k-stationary">#</a></h4>
<ul>
<li>论文比较了两种做 SDDMM(Q·Kᵀ) 的数据流：
S-stationary：把注意力得分“空间映射”到 PE 阵列，每个 PE 算一个分数——这对稀疏不友好：PE 利用率低、控制/重构开销大、还要在 PE 寄存器里存大量中间部分和做 intra-PE 累加。代表作 Sanger 用的就是它。
K-stationary：按列生成注意力分数，K 向量重用充分、中间缓冲小、且只按稀疏索引配对 Q/K 做乘法，天生更适合稀疏。但缺点是Q 访问更频繁——论文说这个缺点由 AE 压缩来缓解（少搬 Q）。结论：选 K-stationary。</li>
</ul>
<h4 id="two-pronged">Two-pronged<a hidden class="anchor" aria-hidden="true" href="#two-pronged">#</a></h4>
<ul>
<li>加速器由两套独立计算引擎组成：
Denser Engine 负责 SDDMM 的“取样致密”部分和后续 SpMM 的 S·V；
Sparser Engine 负责剩余的高度稀疏部分；
两路有独立输出缓冲并行写回；芯片内还集成 Encoder/Decoder 引擎去配合 AE，先压再搬、到阵列前再解码。</li>
</ul>
<h4 id="denser-engine">Denser Engine<a hidden class="anchor" aria-hidden="true" href="#denser-engine">#</a></h4>
<ul>
<li>
<p>动态 PE 资源划分：不同层/头的全局 token 数不同，利用已知的固定 mask预估工作量，把 PE/MAC 按比例在 Denser/Sparser 之间分配。</p>
</li>
<li>
<p>并行与切片：各注意力头并行，但单行 PE 线不足以一拍完成 Q·Kᵀ，因此做细粒度切片并设计时空映射：</p>
<ul>
<li>SDDMM(Q·Kᵀ)：采用 K-stationary，在特征维对 Q/K 切片并空间映射到 PEs，时间上让同一个 K依次与相关的 Q 相乘，并在 inter-PE 方向做部分和累加（按列生成注意力）。
<ul>
<li>SpMM(S·V)：转为 output-stationary，在token 维切片并空间映射，时间上沿特征维累加intra-PE 部分和，减少对注意力图的反复加载、显著降低 on-chip 缓冲压力。两种模式之间需要在 PE 线级别切换“inter-PE ↔ intra-PE”累加。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="sparser-engine">Sparser Engine<a hidden class="anchor" aria-hidden="true" href="#sparser-engine">#</a></h4>
<ul>
<li>
<p>稀疏度可达 &gt;90%，采用 CSC 索引格式预存非零位置，按列（契合 K-stationary 的“按列产出”）取索引，仅加载所需 Q/K；计算只遍历非零。</p>
</li>
<li>
<p>Query-based Q forwarding：两路并行时，Sparser 侧需要的某些 Q 很可能 Denser 侧正在用，因此先查 Denser 的 Q 缓冲再决定是否从 off-chip 取，按需查询降低带宽。其余时空映射策略与 Denser 一致。</p>
</li>
<li>
<p>两路都内置 SoftMax 单元（计算完成单个分数后做 exp）和激活单元（ReLU 用门控，其他用 LUT）。</p>
</li>
</ul>
<h4 id="encoderdecoder-ae-on-chip">Encoder/Decoder （AE on-chip）<a hidden class="anchor" aria-hidden="true" href="#encoderdecoder-ae-on-chip">#</a></h4>
<ul>
<li>为配合 AE，芯片上实现了独立的 Encoder/Decoder 引擎（权重很小，如 6×3，可常驻片上）。Encoder 在 Q/K 线性投影之后立即启用，把 Q/K 压缩后再写回 off-chip；Decoder 在加载入阵列前恢复维度。两者都能和数据搬运全流水重叠，空闲时其 PE 线可复用于其他计算。</li>
</ul>
<h4 id="on-chip-buffer-and-control-module">on-chip buffer and control module<a hidden class="anchor" aria-hidden="true" href="#on-chip-buffer-and-control-module">#</a></h4>
<ul>
<li>两路引擎都配置了专用缓冲：输出(OBuf)、权重(WBuf)、K/S 缓冲、索引缓冲(IdxBuf)、Q/V 缓冲(Q/V Buf)，多端口并行读写以增强复用；矩阵乘法控制器可在致密/稀疏两类负载间切换；并带 SoftMax/Activation 功能单元。</li>
</ul>
<h4 id="编译与可重构algorithm-hardware-interface">编译与可重构（Algorithm-Hardware Interface）<a hidden class="anchor" aria-hidden="true" href="#编译与可重构algorithm-hardware-interface">#</a></h4>
<ul>
<li>给定经过 ViTCoD 训练后的稀疏 ViT 层，网络解析器抽取“全局 token 数、缓冲大小、数据流”等配置，交给硬件编译器生成指令，指导加速器在 Denser/Sparser 之间重分配 on-chip 内存和 PE/MAC，并在 Q·K 与 S·V 两阶段切换 inter-PE ↔ intra-PE 累加模式。一次编译、多次复用摊薄重构成本。</li>
</ul>
<h4 id="端到端数据流推理时一次注意力的落地版">端到端数据流（推理时，一次注意力的“落地版”）<a hidden class="anchor" aria-hidden="true" href="#端到端数据流推理时一次注意力的落地版">#</a></h4>
<ol>
<li>线性投影→Encoder 压缩（Q/K），回写 off-chip；2) 取下一步需要的 Q/K，Decoder 解压进入片上；3) 依据 reorder 的顺序与固定 mask，把工作拆到两路：</li>
</ol>
<ul>
<li>
<p>Denser：按 K-stationary 做 Q·Kᵀ（inter-PE 累加），再以 output-stationary 做 S·V（intra-PE 累加）；</p>
</li>
<li>
<p>Sparser：用 CSC 索引只算非零；需要 Q 时先查询 Denser 的 Q 缓冲再决定外取；</p>
</li>
</ul>
<ol>
<li>两路各自写入独立输出缓冲并行回写/合并。</li>
</ol>
<h4 id="key">key<a hidden class="anchor" aria-hidden="true" href="#key">#</a></h4>
<ul>
<li>
<p>PE 累加模式切换：SDDMM 用 inter-PE（跨 PE 聚合列方向部分和），SpMM 用 intra-PE（每个 PE 内聚合输出），两阶段在同一 PE 线重配置，这是论文强调的“从 K-stationary（Q·K）切到 output-stationary（S·V）”。</p>
</li>
<li>
<p>为什么 K-stationary 能跑得好：K 被充分重用、中间缓冲更小、且天然匹配“按列产出 + 稀疏按列索引”的实现；其“Q 访问更频繁”的缺点由 AE 抵消。</p>
</li>
<li>
<p>Sparser 的索引与转发：用 CSC 预存非零列索引（配合按列产出），并用 Query-based Q forwarding 在两路间共享 Q，减少 off-chip 访问。</p>
</li>
</ul>
<blockquote>
<p>硬件平台参数（论文实现）：面积约 <strong>3 mm²</strong>，DDR4-2400 带宽 <strong>76.8 GB/s</strong>，功耗 <strong>323.9 mW@500 MHz</strong>，片上 <strong>320 KB SRAM</strong>。</p></blockquote>
<h3 id="limitations--weakness--further-research-2">Limitations / Weakness / Further research<a hidden class="anchor" aria-hidden="true" href="#limitations--weakness--further-research-2">#</a></h3>
<hr>
<ol>
<li><strong>局限性分析</strong></li>
</ol>
<p>(1) 方法层面（S&amp;C + AE）</p>
<ul>
<li>
<p><strong>依赖固定 Mask</strong></p>
<ul>
<li>SC（Split &amp; Conquer）的 reorder + prune 依赖一个在 fine-tuning 阶段学到的固定稀疏模式。
<ul>
<li>对于输入分布变化大或 domain shift 明显的任务，固定 Mask 可能导致性能退化。</li>
<li>例如场景变化剧烈（不同类别/布局的图片）时，固定稀疏模式可能错过关键信息。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>无法动态适配注意力模式</strong></p>
<ul>
<li>一旦 mask 固定，推理阶段不再根据输入图片动态生成稀疏模式，这在一些需要局部自适应注意力的任务（如物体检测、多目标跟踪）可能限制性能。</li>
</ul>
</li>
<li>
<p><strong>AE 压缩只针对 Q/K，不覆盖 V 和中间结果</strong></p>
<ul>
<li>带宽瓶颈可能在中间阶段转移到 V 或 S·V 阶段，而 AE 主要压缩了 Q/K。</li>
</ul>
</li>
<li>
<p><strong>Reorder 对多层 ViT 的全局一致性影响未深挖</strong></p>
<ul>
<li>不同层的注意力模式差异很大，文中似乎是对单层 mask 进行优化，但对跨层 mask 复用的影响分析不够深入。</li>
</ul>
</li>
</ul>
<hr>
<p>(2) 硬件实现层面</p>
<ul>
<li>
<p><strong>双引擎（Dense/Sparse PE）资源利用率问题</strong></p>
<ul>
<li>在某些稀疏度分布下，sparse engine 的利用率可能下降，而 dense engine 可能闲置或饱和，这导致硬件资源不均衡。
<ul>
<li>适合固定比例 dense/sparse 的任务，但若稀疏比例波动，性能可能不稳定。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>CSC 稀疏格式存储开销</strong></p>
<ul>
<li>CSC 对稀疏度高的情况非常好，但如果后续模型稀疏度下降，索引存储开销相对增加。</li>
</ul>
</li>
<li>
<p><strong>缺乏多任务并行调度机制</strong></p>
<ul>
<li>当前 pipeline 面向单路 attention，VLM/多模态任务往往有多路 cross-attention，需要调度多个 attention kernel 并行工作。</li>
</ul>
</li>
</ul>
<hr>
<p>(3) 任务/应用层面</p>
<ul>
<li>
<p><strong>对下游任务泛化未验证</strong></p>
<ul>
<li>主要验证是分类任务（ImageNet），在检测、分割、视频理解等任务上的表现未深入研究。
<ul>
<li>稀疏模式在需要保留空间结构信息的任务（比如 dense prediction）可能要重新设计。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>对 Token 语义的敏感性不足</strong></p>
<ul>
<li>Reorder 过程是基于 attention map 的排序，而不是直接考虑 token 的语义（比如物体边界、文本区域等）。</li>
</ul>
</li>
</ul>
<hr>
<ol>
<li><strong>未来可能 Follow 的方向</strong></li>
</ol>
<p>方法改进</p>
<ol>
<li>
<p><strong>动态可调稀疏模式</strong></p>
<ul>
<li>在推理时根据输入图片的低成本特征（如低分辨率 attention map）调整 sparse block 的 mask。
<ul>
<li>可引入轻量的 Gumbel-Softmax / Top-K 筛选器实现动态更新。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>跨层稀疏模式协同优化</strong></p>
<ul>
<li>考虑多层 attention pattern 的相关性，在训练阶段优化一个跨层共享的压缩/稀疏策略，减少 mask 存储。</li>
</ul>
</li>
<li>
<p><strong>多分辨率/分块重排</strong></p>
<ul>
<li>在 reorder 过程中融合多尺度 token 信息，让 global block 更好地覆盖多尺寸目标。</li>
</ul>
</li>
<li>
<p><strong>全路径压缩</strong></p>
<ul>
<li>AE 不仅压缩 Q/K，还压缩 V 及中间 S 矩阵（可以用低秩分解），进一步降低带宽。</li>
</ul>
</li>
</ol>
<hr>
<p>硬件优化</p>
<ol>
<li>
<p><strong>弹性双引擎调度</strong></p>
<ul>
<li>根据实际稀疏比例动态调整 dense/sparse engine 的分配比例，提升资源利用率。</li>
</ul>
</li>
<li>
<p><strong>多路 Attention 并行化</strong></p>
<ul>
<li>针对 VLM 中的 cross-attention、image-text attention，设计多路并行的 sparse/dense 计算通道。</li>
</ul>
</li>
<li>
<p><strong>新稀疏存储格式</strong></p>
<ul>
<li>针对固定 mask，可预编译成 PE-friendly 的压缩布局，减少索引访问延迟。</li>
</ul>
</li>
</ol>
<hr>
<ol>
<li><strong>在 VLM 时代的适配/改进点</strong></li>
</ol>
<p>在 Vision-Language Models 中，ViTCoD 这类结构化稀疏 + 压缩技术仍然有用，但要解决以下问题：</p>
<ol>
<li>
<p><strong>Cross-attention 的稀疏模式不同</strong></p>
<ul>
<li>VLM 中 image-to-text attention 与 image self-attention 的稀疏分布差异很大。
<ul>
<li>需要对不同类型的 attention 分别设计 mask，或者做多模态联合 mask 学习。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Token 长度更长</strong></p>
<ul>
<li>VLM 往往输入长文本 + 高分辨率图像，token 数量可达数千甚至上万，稀疏化带来的收益会更明显，但 mask 存储/调度更复杂。</li>
</ul>
</li>
<li>
<p><strong>多模态 token 排序问题</strong></p>
<ul>
<li>现有 reorder 针对视觉 token，如果混合了文本 token，需要保持跨模态 token 对齐，否则 cross-attention 信息可能受损。</li>
</ul>
</li>
<li>
<p><strong>对齐/语义保持</strong></p>
<ul>
<li>VLM 强调图文对齐，reorder 如果破坏视觉 token 与文本 token 的语义对应，会降低模型性能，需要加入对齐约束。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="vit-slice-end-to-end-vision-transformer-accelerator-with-bit-slice-algorithm"><strong>ViT-slice: End-to-end Vision Transformer Accelerator with Bit-slice Algorithm</strong><a hidden class="anchor" aria-hidden="true" href="#vit-slice-end-to-end-vision-transformer-accelerator-with-bit-slice-algorithm">#</a></h2>
<hr>
<h2 id="fnm-trans-efficient-fpga-based-transformer-architecture-with-full-nm-sparsity"><strong>FNM-Trans: Efficient FPGA-based Transformer Architecture with Full N:M Sparsity</strong><a hidden class="anchor" aria-hidden="true" href="#fnm-trans-efficient-fpga-based-transformer-architecture-with-full-nm-sparsity">#</a></h2>
<hr>
<h2 id="hg-pipe-vision-transformer-acceleration-with-hybrid-grained-pipeline"><strong>HG-PIPE: Vision Transformer Acceleration with Hybrid-Grained Pipeline</strong><a hidden class="anchor" aria-hidden="true" href="#hg-pipe-vision-transformer-acceleration-with-hybrid-grained-pipeline">#</a></h2>
<hr>
<h2 id="fas-trans-fully-exploiting-ffn-and-attention-sparsity-for-transformer-on-fpga"><strong>FAS-Trans: Fully Exploiting FFN and Attention Sparsity for Transformer on FPGA</strong><a hidden class="anchor" aria-hidden="true" href="#fas-trans-fully-exploiting-ffn-and-attention-sparsity-for-transformer-on-fpga">#</a></h2>
<hr>
<h2 id="fact-ffn-attention-co-optimized-transformer-architecture-with-eager-correlation-prediction"><strong>FACT: FFN-Attention Co-optimized Transformer Architecture with Eager Correlation Prediction</strong><a hidden class="anchor" aria-hidden="true" href="#fact-ffn-attention-co-optimized-transformer-architecture-with-eager-correlation-prediction">#</a></h2>
<h2 id="image-4"><img alt="image" loading="lazy" src="/blogs/images/TpsdbnDStoCzBSxZFhhcYIMFnsc.png"><a hidden class="anchor" aria-hidden="true" href="#image-4">#</a></h2>
<h3 id="background--target-4">Background &amp; Target<a hidden class="anchor" aria-hidden="true" href="#background--target-4">#</a></h3>
<hr>
<h2 id="while-the-attention-computation-focused-by-most-previous-works-only-has-decent-power-share-when-dealing-with-extremely-long-inputs-fact-an-efficient-algorithm-hardware-co-design-optimizing-all-three-modules-of-transformer">While the attention computation, focused by most previous works, only has decent power share when dealing with extremely long inputs. FACT, an efficient algorithm-hardware co-design optimizing all three modules of Transformer<a hidden class="anchor" aria-hidden="true" href="#while-the-attention-computation-focused-by-most-previous-works-only-has-decent-power-share-when-dealing-with-extremely-long-inputs-fact-an-efficient-algorithm-hardware-co-design-optimizing-all-three-modules-of-transformer">#</a></h2>
<h3 id="algorithm-5">Algorithm<a hidden class="anchor" aria-hidden="true" href="#algorithm-5">#</a></h3>
<hr>
<h4 id="ep--eager-prediction">EP &ndash; eager prediction<a hidden class="anchor" aria-hidden="true" href="#ep--eager-prediction">#</a></h4>
<p>If there exists a few large probabilities in the 𝑆, the rest are very small and can be safely skipped since they have little impact on the output.</p>
<p>Generating the QK matrices causes much more computation and power than the _𝑄 _· _𝐾 𝑇 _, leading to suboptimal improvement</p>
<p>EP with cross-stage log-based inner-product estimation, which can reduce not only attention score computation but also the _𝑄𝐾𝑉  _linear projection.</p>
<h4 id="attention-distribution-aware-qkv-generation">Attention-distribution-aware QKV Generation<a hidden class="anchor" aria-hidden="true" href="#attention-distribution-aware-qkv-generation">#</a></h4>
<p>在 $$
Eager\ Generation
$$
的时候，对于预测注意力分数矩阵$$
\tilde{A}~
$$
,每一行会取 $$
top-k
$$
做filter，然后 $$
non-top-k
$$
呢就直接skip掉，那么由于这样形成的一个跟 $$
\tilde{A}
$$
维度相同的mask矩阵自然会形成一定的sparsity，比如 $$
\tilde{A}~
$$
的某一列全部都被skip掉的话，那么对应反推，在 $$
Q\ K\ V\ generation
$$
的时候，利用 $$
\tilde{A}~
$$
产生的sparsity mask就可以忽略掉一部分的精确计算，从而实现 $$
Q K V\ generation / FFN\ layers
$$
更好的加速。</p>
<dl>
<dt>$$</dt>
<dt>KV\ sparsity</dt>
<dt>$$</dt>
<dd>It derives directly from the top-k result of the predicted _𝐴_ˆ matrix. If a column in the _𝐴_ˆ matrix has no values selected by the top-k, the key tensor related to this column is no longer required and can be safely pruned. Similarly, since the V matrix is multiplied by the attention matrix, the row with the same index in V matrix has no effect on the output, either, and can be safely removed.</dd>
<dt>$$</dt>
<dt>Q\ sparsity</dt>
<dt>$$</dt>
<dd>Hence, when the _𝐴_ˆ matrix is obtained from EP, the difference between the 1 _𝑠𝑡 _and 2 _𝑛𝑑 _value of each row is compared to a threshold (we choose 3 based on experiments). If the former is larger, EP regards this row as being dominated by the largest token and directly uses a one-hot tensor as the _softmax _result where the largest token is assigned with 1.0 probability. In this way, the QK generation and attention computation related to this row can be fully skipped, and all that is needed is to copy the corresponding V tensor as the output.</dd>
</dl>
<h4 id="token-wise-mixed-precision-ffn-computation">Token-wise Mixed Precision FFN computation<a hidden class="anchor" aria-hidden="true" href="#token-wise-mixed-precision-ffn-computation">#</a></h4>
<hr>
<h3 id="hardware-3">Hardware<a hidden class="anchor" aria-hidden="true" href="#hardware-3">#</a></h3>
<hr>
<h4 id="ep-unit-with-lod-circuit">EP unit with LOD circuit<a hidden class="anchor" aria-hidden="true" href="#ep-unit-with-lod-circuit">#</a></h4>
<h4 id="kv-differential-order-a-scheduler-to-better-match-with-the-ep-algorithm">KV-differential order (a scheduler to better match with the EP algorithm)<a hidden class="anchor" aria-hidden="true" href="#kv-differential-order-a-scheduler-to-better-match-with-the-ep-algorithm">#</a></h4>
<h4 id="diagonal-storage-pattern-for-mixed-precision-ffn">Diagonal Storage Pattern for Mixed Precision FFN<a hidden class="anchor" aria-hidden="true" href="#diagonal-storage-pattern-for-mixed-precision-ffn">#</a></h4>
<hr>
<h3 id="limitations--weakness--further-research-3">Limitations / Weakness / Further research<a hidden class="anchor" aria-hidden="true" href="#limitations--weakness--further-research-3">#</a></h3>
<hr>
<p>The design concept of FACT and EP is via predicting the redundancy before computation, thus skipping unnecessary computation. FACT’s EP is an output dynamic sparsity method. Further, EP prediction is a unique cross-stage method for Transformer</p>
<p>感觉token importance的评估方法可以有所改进，尤其是FFN的这个加速方法是不是应该有更合理的判断方法？不知道是否是真的有效果</p>
<p>另外QKV的sparse部分感觉做的很不错，可以有参考，但是是按照token-wise进行的，直接舍弃掉完整token对应的Q/KV的generation，那么如果带入到多模态里面的话，直接进行这样成token的prune会不会掉点比较严重。</p>
<h2 id="breaking-the-low-rank-dilemma-of-linear-attention"><strong>Breaking the Low-Rank Dilemma of Linear Attention</strong><a hidden class="anchor" aria-hidden="true" href="#breaking-the-low-rank-dilemma-of-linear-attention">#</a></h2>
<hr>
<h2 id="falcon-resolving-visual-redundancy-and-fragmentation-in-high-resolution-multimodal-large-language-models-via-visual-registers"><strong>FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers</strong><a hidden class="anchor" aria-hidden="true" href="#falcon-resolving-visual-redundancy-and-fragmentation-in-high-resolution-multimodal-large-language-models-via-visual-registers">#</a></h2>
<hr>
<h2 id="qwen25-vl-technical-report"><strong>Qwen2.5-VL Technical Report</strong><a hidden class="anchor" aria-hidden="true" href="#qwen25-vl-technical-report">#</a></h2>
<hr>
<h3 id="window-attention">window attention<a hidden class="anchor" aria-hidden="true" href="#window-attention">#</a></h3>
<h4 id="computation-complexity">computation complexity<a hidden class="anchor" aria-hidden="true" href="#computation-complexity">#</a></h4>
<ol>
<li>self-attention</li>
</ol>
<p>$$
\Omega(MSA) = 4HW\times d_h\times d_i + 2(HW)^2 \times d_i\
$$</p>
<ol>
<li>window-self-attention</li>
</ol>
<p>$$
\Omega(\text{1-MSA}) = 4M^2\times d_h\times d_i + 2({M^2})^2 \times d_i\
$$</p>
<p>$$
\Omega(\text{W-MSA}) = 4HW\times d_h\times d_i + 2M^2HW\times d_i= HW \times(4d_hd_i+2M^2d_i)\
$$</p>
<p>window attention不等价于对MSA做tiling，WMSA是算法级别的注意力掩码（mask），它硬性屏蔽了跨窗口的 token 交互。即便你用相同的 M×M 大小切 patch，如果是全局 Attention，就还是能跨 patch 互相注意；Window Attention 则不行。</p>
<p>能否有纯硬件调度来支持attention的计算复杂度的降低？可能类似于window MSA这种么，Flashattention就是纯HBM➕算法调度的，但是flashattention基本上做烂了</p>
<h2 id="blip-2-bootstrapping-language-image-pre-training-with-frozen-image-encoders-and-large-language-models">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models<a hidden class="anchor" aria-hidden="true" href="#blip-2-bootstrapping-language-image-pre-training-with-frozen-image-encoders-and-large-language-models">#</a></h2>
<hr>
<h2 id="divprune-diversity-based-visual-token-pruning-for-large-multimodal-models">DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models<a hidden class="anchor" aria-hidden="true" href="#divprune-diversity-based-visual-token-pruning-for-large-multimodal-models">#</a></h2>
<hr>
<p>非常取巧的一篇文章，没有按照常规的比如判断importance的方法，而是换了一种prune的思路，采用最大化最小距离，换句话说就是用贪心算法取Visual token的特定元素数量的subset，使得最终该subset内的元素之间的距离最大化（最多样性化）</p>
<h2 id="todre-visual-token-pruning-via-diversity-and-task-awareness-for-efficient-large-vision-language-models">ToDRE: Visual Token Pruning via Diversity and Task Awareness for Efficient Large Vision-Language Models<a hidden class="anchor" aria-hidden="true" href="#todre-visual-token-pruning-via-diversity-and-task-awareness-for-efficient-large-vision-language-models">#</a></h2>
<p>作者不是很distinguished</p>
<h3 id="background--target-5">Background &amp; Target<a hidden class="anchor" aria-hidden="true" href="#background--target-5">#</a></h3>
<p>同样是考虑diversity的一个prune思路，(cite了上面的DivPrune），额外还有token-task relevance的一个维度，根据两个维度进行prune</p>
<h3 id="algorithm-6">Algorithm<a hidden class="anchor" aria-hidden="true" href="#algorithm-6">#</a></h3>
<h2 id="fcot-vladvancing-text-oriented-large-vision-language-models-with-efficient-visual-token-compression"><strong>FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression</strong><a hidden class="anchor" aria-hidden="true" href="#fcot-vladvancing-text-oriented-large-vision-language-models-with-efficient-visual-token-compression">#</a></h2>
<hr>
<h2 id="atp-llava-adaptive-token-pruning-for-large-vision-language-models"><strong>ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models</strong><a hidden class="anchor" aria-hidden="true" href="#atp-llava-adaptive-token-pruning-for-large-vision-language-models">#</a></h2>
<hr>
<h2 id="treat-visual-tokens-as-text-but-your-mllm-only-needs-fewer-efforts-to-see"><strong>Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to See</strong><a hidden class="anchor" aria-hidden="true" href="#treat-visual-tokens-as-text-but-your-mllm-only-needs-fewer-efforts-to-see">#</a></h2>
<hr>
<h2 id="window-token-concatenation-for-efficient-visual-large-language-models"><strong>Window Token Concatenation for Efficient Visual Large Language Models</strong><a hidden class="anchor" aria-hidden="true" href="#window-token-concatenation-for-efficient-visual-large-language-models">#</a></h2>
<hr>
<h2 id="star-stage-wise-attention-guided-token-reduction-for-efficient-large-vision-language-models-inference"><strong>STAR: Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language Models Inference</strong><a hidden class="anchor" aria-hidden="true" href="#star-stage-wise-attention-guided-token-reduction-for-efficient-large-vision-language-models-inference">#</a></h2>
<hr>
<h2 id="beyond-attention-or-similarity-maximizing-conditional-diversity-for-token-pruning-in-mllms"><strong>Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs</strong><a hidden class="anchor" aria-hidden="true" href="#beyond-attention-or-similarity-maximizing-conditional-diversity-for-token-pruning-in-mllms">#</a></h2>
<hr>
<h2 id="sanger-a-co-design-framework-for-enabling-sparse-attention-using-reconfigurable-architecture"><strong>Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture</strong><a hidden class="anchor" aria-hidden="true" href="#sanger-a-co-design-framework-for-enabling-sparse-attention-using-reconfigurable-architecture">#</a></h2>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blogs/posts/prml-dl/">
    <span class="title">« Prev</span>
    <br>
    <span>PRML-DL</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share VLM pruning : Hw and Alg co-design on x"
            href="https://x.com/intent/tweet/?text=VLM%20pruning%20%3a%20Hw%20and%20Alg%20co-design&amp;url=http%3a%2f%2flocalhost%3a1313%2fblogs%2fposts%2fvlm-pruning--hw-and-alg-co-design%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share VLM pruning : Hw and Alg co-design on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fblogs%2fposts%2fvlm-pruning--hw-and-alg-co-design%2f&amp;title=VLM%20pruning%20%3a%20Hw%20and%20Alg%20co-design&amp;summary=VLM%20pruning%20%3a%20Hw%20and%20Alg%20co-design&amp;source=http%3a%2f%2flocalhost%3a1313%2fblogs%2fposts%2fvlm-pruning--hw-and-alg-co-design%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share VLM pruning : Hw and Alg co-design on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fblogs%2fposts%2fvlm-pruning--hw-and-alg-co-design%2f&title=VLM%20pruning%20%3a%20Hw%20and%20Alg%20co-design">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share VLM pruning : Hw and Alg co-design on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fblogs%2fposts%2fvlm-pruning--hw-and-alg-co-design%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share VLM pruning : Hw and Alg co-design on whatsapp"
            href="https://api.whatsapp.com/send?text=VLM%20pruning%20%3a%20Hw%20and%20Alg%20co-design%20-%20http%3a%2f%2flocalhost%3a1313%2fblogs%2fposts%2fvlm-pruning--hw-and-alg-co-design%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share VLM pruning : Hw and Alg co-design on telegram"
            href="https://telegram.me/share/url?text=VLM%20pruning%20%3a%20Hw%20and%20Alg%20co-design&amp;url=http%3a%2f%2flocalhost%3a1313%2fblogs%2fposts%2fvlm-pruning--hw-and-alg-co-design%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share VLM pruning : Hw and Alg co-design on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=VLM%20pruning%20%3a%20Hw%20and%20Alg%20co-design&u=http%3a%2f%2flocalhost%3a1313%2fblogs%2fposts%2fvlm-pruning--hw-and-alg-co-design%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/blogs/">LArielO&#39;s LAB</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
